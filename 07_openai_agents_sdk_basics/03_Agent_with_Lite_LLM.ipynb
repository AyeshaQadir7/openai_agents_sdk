{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Agent with Lite LLM**"
      ],
      "metadata": {
        "id": "DJNrz8LFio8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is LiteLLM?**\n",
        "\n",
        "👉 **LiteLLM** is an **open-source Python library** that acts like a **universal translator / wrapper** for different LLM providers.\n",
        "\n",
        "* Normally, every LLM API (OpenAI, Anthropic, Google Gemini, Cohere, etc.) has **different syntax** and ways to call models.\n",
        "* LiteLLM solves this by giving you **one simple interface** to call *any* LLM, regardless of provider.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## **Simple Analogy**\n",
        "\n",
        "Imagine you travel to many countries.\n",
        "\n",
        "* Each country speaks a different language.\n",
        "* Without LiteLLM → you need to learn French, Spanish, Japanese separately.\n",
        "* With LiteLLM → you have a **universal translator**. You speak one language (LiteLLM syntax), and it handles the translation for you.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "# **How LiteLLM Works**\n",
        "\n",
        "* You install it with:\n",
        "\n",
        "  ```bash\n",
        "  pip install litellm\n",
        "  ```\n",
        "\n",
        "* Then, instead of learning each provider’s SDK, you use **the same function**:\n",
        "\n",
        "```python\n",
        "from litellm import completion\n",
        "\n",
        "# Call OpenAI GPT\n",
        "response = completion(model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": \"Hello!\"}])\n",
        "\n",
        "# Call Anthropic Claude\n",
        "response = completion(model=\"claude-3-opus\", messages=[{\"role\": \"user\", \"content\": \"Hello!\"}])\n",
        "\n",
        "# Call Google Gemini\n",
        "response = completion(model=\"gemini-pro\", messages=[{\"role\": \"user\", \"content\": \"Hello!\"}])\n",
        "```\n",
        "\n",
        "Notice 👉 the code looks **the same** for all models.\n",
        "Only the `model` name changes.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "# **Benefits of LiteLLM**\n",
        "\n",
        "1. **One unified API**\n",
        "\n",
        "   * No need to learn 10 different LLM SDKs.\n",
        "   * Write once, run everywhere.\n",
        "\n",
        "2. **Supports 100+ Models**\n",
        "\n",
        "   * OpenAI (GPT), Anthropic (Claude), Google (Gemini), Cohere, HuggingFace, local models, etc.\n",
        "\n",
        "3. **Drop-in replacement for OpenAI SDK**\n",
        "\n",
        "   * It mimics the OpenAI API format (`chat.completions`), so your existing OpenAI code runs with other models easily.\n",
        "\n",
        "4. **Extra Features**\n",
        "\n",
        "   * Logging & monitoring\n",
        "   * Rate limiting & retries\n",
        "   * Cost tracking (helps if you mix providers)\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "# **LiteLLM vs. OpenAI Agents SDK**\n",
        "\n",
        "* **LiteLLM** = **Compatibility Layer**\n",
        "\n",
        "  * Helps you swap and use different LLM providers easily.\n",
        "  * It’s about *access* to models.\n",
        "\n",
        "* **Agents SDK** = **Agent Framework**\n",
        "\n",
        "  * Helps you build *agents* (with memory, tools, guardrails, multi-agent teamwork).\n",
        "  * It’s about *intelligence + orchestration*.\n",
        "\n",
        "✅ You can even use **LiteLLM + Agents SDK together**:\n",
        "\n",
        "* LiteLLM gives you model flexibility.\n",
        "* Agents SDK gives you the full agentic framework.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# **Quick Recap**\n",
        "\n",
        "* **LiteLLM** = Universal translator for LLM APIs.\n",
        "* Lets you call **any model** (GPT, Claude, Gemini, Cohere, etc.) with the **same code**.\n",
        "* Saves dev time + avoids vendor lock-in.\n",
        "* Best used if you want **multi-model support** or to **switch providers easily**.\n"
      ],
      "metadata": {
        "id": "I62mniHbidWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pFlm_ZZTHii8",
        "outputId": "9c9ea2b5-690f-46d4-c07e-2e0f13ef87d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.0\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.12/dist-packages (from openai==0.28.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai==0.28.0) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from openai==0.28.0) (3.12.15)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28.0) (2025.8.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28.0) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->openai==0.28.0) (4.14.1)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.100.0\n",
            "    Uninstalling openai-1.100.0:\n",
            "      Successfully uninstalled openai-1.100.0\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq openai-agents \"openai-agents[litellm]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRUZYJsPmFFN",
        "outputId": "10412239-1598-4c53-ffba-fac85c37b7c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.0/812.0 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio #making jupyter notebook capable of running asnchronous functions\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "yGfyYTPCmxd7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "pPqR2k_3GaJ4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "API_KEY = userdata.get(\"GEMINI_API_KEY\") # must give\n",
        "MODEL_NAME = \"gemini/gemini-2.5-flash\" # must give\n",
        "\n",
        "assistant = Agent(\n",
        "    name= \"Assistant\",\n",
        "    instructions= \"You will help with the queries\",\n",
        "    model = LitellmModel (model= MODEL_NAME, api_key= API_KEY) # overwriting the llm\n",
        ")\n",
        "\n",
        "result = Runner.run_sync(starting_agent = assistant, input= \"Hello, How are you?\"\n",
        ")\n",
        "\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MZTj05DFpeB",
        "outputId": "316b9add-5c88-431e-fd7e-88325e8e2500"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! As an AI, I don't have feelings in the way humans do, but I'm ready to assist you. How can I help you today?\n"
          ]
        }
      ]
    }
  ]
}