{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "üìì [Open in Colab][1]\n",
        "\n",
        "[1]: https://colab.research.google.com/drive/1kreAw3ZRqfmi8P9_O6-bv5NFeAxPyT5u?usp=sharing"
      ],
      "metadata": {
        "id": "4HIucj8FjEbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Streaming in LLMs & Agents SDK**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **What is Streaming?**\n",
        "\n",
        "* Normally, when you ask an LLM a question, the **server generates the entire response first** and then sends it back in one chunk.\n",
        "\n",
        "* With **streaming**, the response is sent **piece by piece (token by token)** as it‚Äôs generated.\n",
        "\n",
        "* You don‚Äôt wait for the full answer ‚Üí you see it **in real time**.\n",
        "\n",
        "üëâ Think of it like:\n",
        "\n",
        "* **Non-streaming** = getting a complete email after the sender finishes writing.\n",
        "* **Streaming** = watching the sender type the email letter by letter in real time.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 2. **How Streaming Works**\n",
        "\n",
        "1. **User sends a request** ‚Üí ‚ÄúWrite me a summary of AI.‚Äù\n",
        "2. LLM **starts generating tokens** (words, subwords).\n",
        "3. Instead of waiting, the **server streams tokens immediately** to the client.\n",
        "4. The client (your app) **renders the output progressively** (like a typing effect).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 3. **Streaming in OpenAI Agents SDK**\n",
        "\n",
        "In the **Agents SDK**, streaming can happen at **two levels**:\n",
        "\n",
        "1. **Model response streaming**\n",
        "\n",
        "   * You get the assistant‚Äôs text **as it‚Äôs being generated**.\n",
        "   * Useful for chatbots ‚Üí feels fast and natural.\n",
        "\n",
        "2. **Event streaming (Agent workflow)**\n",
        "\n",
        "   * You don‚Äôt just see text ‚Äî you also see **events**:\n",
        "\n",
        "     * When the agent decides to call a tool.\n",
        "     * When tool results come back.\n",
        "     * When the agent continues reasoning.\n",
        "   * Lets developers **visualize and debug** how the agent is thinking.\n",
        "\n",
        "üëâ This is more advanced than just text streaming.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 4. **Why Streaming is Important**\n",
        "\n",
        "* ‚úÖ **Faster user experience** ‚Üí Users see answers instantly, not after 10s.\n",
        "* ‚úÖ **Transparency** ‚Üí Developers can see intermediate steps (tools used, reasoning).\n",
        "* ‚úÖ **Interactivity** ‚Üí You can **interrupt**, **cancel**, or **update UI live**.\n",
        "* ‚úÖ **Scalability** ‚Üí Useful in chatbots, copilots, dashboards.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 5. **Use Cases of Streaming**\n",
        "\n",
        "* **Chatbots / Virtual Assistants** ‚Üí Feels like human typing.\n",
        "* **Search engines** ‚Üí Show results progressively.\n",
        "* **Data-heavy tools** ‚Üí Stream analysis while still computing.\n",
        "* **Agents with tools** ‚Üí Show intermediate reasoning (‚ÄúNow fetching weather‚Ä¶ Done!‚Äù).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 6. **Exam Cheat-Sheet**\n",
        "\n",
        "| Aspect            | Non-Streaming üê¢              | Streaming üöÄ                          |\n",
        "| ----------------- | ----------------------------- | ------------------------------------- |\n",
        "| Response Delivery | After full output ready       | Token by token (real-time)            |\n",
        "| User Experience   | Slow, delayed                 | Fast, natural                         |\n",
        "| Visibility        | Only final answer             | Intermediate reasoning + events       |\n",
        "| Best Use Cases    | Small queries, static answers | Chatbots, copilots, real-time systems |\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Summary**:\n",
        "**Streaming** = delivering model output and agent events in **real time**, instead of waiting for the full response.\n",
        "In OpenAI Agents SDK, streaming lets you not only see **partial tokens** but also **agent reasoning and tool calls** ‚Äî making it powerful for building transparent, responsive AI systems.\n"
      ],
      "metadata": {
        "id": "kLFsXdZ8ILpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokens (Short Explanation)**\n",
        "\n",
        "* **Tokens** are the **smallest units of text** an LLM (like GPT) reads or writes.\n",
        "* They are usually **words, parts of words, or even punctuation**.\n",
        "\n",
        "### Examples:\n",
        "\n",
        "* `\"Hello\"` ‚Üí 1 token\n",
        "* `\"ChatGPT is amazing!\"` ‚Üí might be 4 tokens (`Chat`, `GPT`, `is`, `amazing!`)\n",
        "* `\"unbelievable\"` ‚Üí could be split into smaller tokens (`un`, `believable`)\n",
        "\n",
        "---\n",
        "\n",
        "## **Why Tokens Matter?**\n",
        "\n",
        "* LLMs **process text as tokens**, not as full words.\n",
        "* Cost and limits in OpenAI API are measured in **tokens**.\n",
        "* 1 token ‚âà 4 characters in English, or about **¬æ of a word**.\n",
        "\n",
        "---\n",
        "\n",
        "**In short**:\n",
        "Tokens = **text chunks** that LLMs understand.\n",
        "They decide **cost, context length, and response size**.\n"
      ],
      "metadata": {
        "id": "UcP2d9koz-7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Delta** (Short Explanation)\n",
        "\n",
        "* **Delta** = the **newly generated piece of text** the LLM sends during **streaming**.\n",
        "* Instead of sending the full response each time, the model streams **deltas (tokens/chunks)** one by one.\n",
        "* You combine all deltas ‚Üí final response.\n",
        "\n",
        "üëâ Example:\n",
        "\n",
        "* Deltas: `\"Hel\"` ‚Üí `\"lo \"` ‚Üí `\"world!\"`\n",
        "* Final response = `\"Hello world!\"`\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **In short**:\n",
        "**Delta = the latest chunk of text from the stream.**\n"
      ],
      "metadata": {
        "id": "SZROP3SqiQYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq openai-agents"
      ],
      "metadata": {
        "id": "II-zR-heM0Qu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "AFyMuiUwNKSe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, set_default_openai_api, set_default_openai_client, set_tracing_disabled\n",
        "from agents.run import RunConfig\n",
        "from google.colab import userdata\n",
        "\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key = GEMINI_API_KEY,\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    openai_client = external_client,\n",
        ")\n"
      ],
      "metadata": {
        "id": "ifDAn1E3O4SV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "from agents import Agent, Runner\n",
        "\n",
        "async def main():\n",
        "  agent= Agent(\n",
        "      name= \"Joker\",\n",
        "      instructions= \"You are a helpful assistant\",\n",
        "      model= model,\n",
        "  )\n",
        "\n",
        "  result = Runner.run_streamed(agent, input= \"Tell me 5 jokes.\")\n",
        "  print(type (result), result, \"\\n\\n\")\n",
        "  async for event in  result.stream_events():\n",
        "    print(event)\n",
        "\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g7nAYl1sRpyg",
        "outputId": "d8d7ea34-fb36-4507-fb14-9b7c89d57a10"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'agents.result.RunResultStreaming'> RunResultStreaming:\n",
            "- Current agent: Agent(name=\"Joker\", ...)\n",
            "- Current turn: 0\n",
            "- Max turns: 10\n",
            "- Is complete: False\n",
            "- Final output (NoneType):\n",
            "    None\n",
            "- 0 new item(s)\n",
            "- 0 raw response(s)\n",
            "- 0 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResultStreaming` for more details) \n",
            "\n",
            "\n",
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Joker', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions='You are a helpful assistant', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7916b5d5cfe0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1756754536.4581404, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=None, safety_identifier=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=None), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Alright', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\", here are 5 jokes for you:\\n\\n1.  Why don'\", item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=4, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='t scientists trust atoms?\\n    Because they make up everything!\\n\\n2.', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=5, type='response.output_text.delta'), type='raw_response_event')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\"  Parallel lines have so much in common.\\n    It's a shame they'll never meet.\\n\\n3.  Why did the scarecrow win an award?\", item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=6, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='\\n    Because he was outstanding in his field!\\n\\n4.  Why did the bicycle fall over?\\n    Because it was two tired!\\n\\n5.  What', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=7, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' do you call a fish with no eyes?\\n    Fsh!\\n', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=8, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text=\"Alright, here are 5 jokes for you:\\n\\n1.  Why don't scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  Parallel lines have so much in common.\\n    It's a shame they'll never meet.\\n\\n3.  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n4.  Why did the bicycle fall over?\\n    Because it was two tired!\\n\\n5.  What do you call a fish with no eyes?\\n    Fsh!\\n\", type='output_text', logprobs=None), sequence_number=9, type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Alright, here are 5 jokes for you:\\n\\n1.  Why don't scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  Parallel lines have so much in common.\\n    It's a shame they'll never meet.\\n\\n3.  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n4.  Why did the bicycle fall over?\\n    Because it was two tired!\\n\\n5.  What do you call a fish with no eyes?\\n    Fsh!\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), output_index=0, sequence_number=10, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1756754536.4581404, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Alright, here are 5 jokes for you:\\n\\n1.  Why don't scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  Parallel lines have so much in common.\\n    It's a shame they'll never meet.\\n\\n3.  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n4.  Why did the bicycle fall over?\\n    Because it was two tired!\\n\\n5.  What do you call a fish with no eyes?\\n    Fsh!\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=None, safety_identifier=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=11, type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Joker', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions='You are a helpful assistant', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7916b5d5cfe0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Alright, here are 5 jokes for you:\\n\\n1.  Why don't scientists trust atoms?\\n    Because they make up everything!\\n\\n2.  Parallel lines have so much in common.\\n    It's a shame they'll never meet.\\n\\n3.  Why did the scarecrow win an award?\\n    Because he was outstanding in his field!\\n\\n4.  Why did the bicycle fall over?\\n    Because it was two tired!\\n\\n5.  What do you call a fish with no eyes?\\n    Fsh!\\n\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming Text code"
      ],
      "metadata": {
        "id": "adJ0lOpvcYl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "from agents import Agent, Runner\n",
        "\n",
        "async def main():\n",
        "  agent= Agent(\n",
        "      name= \"Joker\",\n",
        "      instructions= \"You are a helpful assistant\",\n",
        "      model= model,\n",
        "  )\n",
        "\n",
        "  result = Runner.run_streamed(agent, input= \"Tell me 5 jokes.\") #: Runs the agent in streaming mode.\n",
        "  async for event in  result.stream_events():\n",
        "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
        "      print(event.data.delta, end=\"\", flush=True) # flush= forces immediate display (no buffering delay).\n",
        "\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQf5pq26a-AT",
        "outputId": "971ff859-bcbe-4b7b-91cb-935a3dc809f4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alright, here are 5 jokes for you:\n",
            "\n",
            "1.  Why don't scientists trust atoms?\n",
            "    Because they make up everything!\n",
            "\n",
            "2.  Parallel lines have so much in common.\n",
            "    It's a shame they'll never meet.\n",
            "\n",
            "3.  Why did the scarecrow win an award?\n",
            "    Because he was outstanding in his field!\n",
            "\n",
            "4.  What do you call a lazy kangaroo?\n",
            "    Pouch potato!\n",
            "\n",
            "5.  Why did the bicycle fall over?\n",
            "    Because it was two tired!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stream item code"
      ],
      "metadata": {
        "id": "QYw_Tpcsgoev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import random\n",
        "\n",
        "from agents import Agent, ItemHelpers, Runner, function_tool\n",
        "\n",
        "@function_tool\n",
        "def how_many_jokes() -> int:\n",
        "  return random.randint(1, 10)\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name= \"Joker\",\n",
        "        instructions= \"First cal the `how_many_jokes` tool, then tell that many jokes\",\n",
        "        tools= [how_many_jokes],\n",
        "        model = model,\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(agent, input= \"Hi\",)\n",
        "\n",
        "    print(\"=== Run Starting ===\")\n",
        "    async for event in result.stream_events():\n",
        "      if event.type == \"raw_response_event\":\n",
        "        continue\n",
        "\n",
        "      elif event.type == \"agent_updated_stream_event\":\n",
        "        print(f\"Agent updated: {event.new_agent.name}\")\n",
        "        continue\n",
        "\n",
        "      elif event.type == \"run_item_stream_event\":\n",
        "        if event.item.type == \"tool_call_item\":\n",
        "          print(\"-- Tool was called\")\n",
        "\n",
        "        elif event.item.type == \"tool_call_output_item\":\n",
        "          print(f\"-- Tool output: {event.item.output}\")\n",
        "        elif event.item.type == \"message_output_item\":\n",
        "          print(f\"-- Message output: \\n {ItemHelpers.text_message_output(event.item)}\")\n",
        "        else:\n",
        "          pass\n",
        "\n",
        "asyncio.run(main())\n",
        "\n",
        "print(\"=== Run complete ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4hGLRtggph3",
        "outputId": "da70d2d4-5052-430c-c0de-6fe9fabd68b1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openai.agents:OPENAI_API_KEY is not set, skipping trace export\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Run Starting ===\n",
            "Agent updated: Joker\n",
            "-- Tool was called\n",
            "-- Tool output: 3\n",
            "-- Message output: \n",
            " OK, I will tell you 3 jokes.\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            " –ø–∞—Ä–∞–ª–ª–µ–ª–µ–ø–∏–ø–µ–¥\n",
            "\n",
            "Why did the scarecrow win an award?\n",
            "\n",
            "Because he was outstanding in his field!\n",
            "\n",
            "Why did the bicycle fall over?\n",
            "\n",
            "Because it was two tired!\n",
            "\n",
            "=== Run complete ===\n"
          ]
        }
      ]
    }
  ]
}