{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQhcggG95yjA"
      },
      "source": [
        "# **OpenAI Agents SDK Basics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8nmoyxR2AOu"
      },
      "source": [
        "## <font color= orange>**Primitives in OpenAI Agents SDK**</font>\n",
        "\n",
        "These are the **building blocks** you need to understand before creating powerful agentic systems. Let‚Äôs go step by step, in a way even someone new to AI can clearly get it.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 1. **Agents**\n",
        "\n",
        "* **What it is**:\n",
        "  An **Agent** is the main brain. It receives a request (user input), thinks about it, decides what to do, and responds.\n",
        "\n",
        "* **Analogy**: Imagine you‚Äôre at a restaurant.\n",
        "\n",
        "  * The **agent = waiter**.\n",
        "  * You (user) tell the waiter your order.\n",
        "  * The waiter understands, checks the menu (knowledge/tools), and brings you food (output).\n",
        "\n",
        "* **In SDK terms**:\n",
        "\n",
        "  * An agent is tied to a **model** (like GPT-4.1).\n",
        "  * You give it **instructions** (personality, rules, role).\n",
        "  * You can connect it to **tools** (like APIs, databases, calculators).\n",
        "\n",
        "‚úÖ **Key Idea**: An **Agent = AI worker** with a job description + tools.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 2. **Handoffs**\n",
        "\n",
        "* **What it is**:\n",
        "  A **handoff** is when one agent passes the task to another agent.\n",
        "\n",
        "* **Analogy**: Think of a **relay race**. One runner (agent) runs part of the race, then passes the baton (task) to the next runner (another agent).\n",
        "\n",
        "* **Why useful?**\n",
        "\n",
        "  * Different agents can specialize in different tasks.\n",
        "  * Example:\n",
        "\n",
        "    * Agent A = Customer support bot.\n",
        "    * Agent B = Billing bot.\n",
        "    * If the customer asks about billing, Agent A **hands off** to Agent B.\n",
        "\n",
        "\n",
        "‚úÖ **Key Idea**: **Handoffs = passing tasks between agents like teammates.**\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 3. **Guardrails**\n",
        "\n",
        "* **What it is**:\n",
        "  **Guardrails** are safety rules or boundaries that agents must follow.\n",
        "\n",
        "* **Analogy**: Like **seatbelts & traffic rules** for a driver. They don‚Äôt stop the car from moving, but they keep it safe.\n",
        "\n",
        "* **Examples**:\n",
        "\n",
        "  * Don‚Äôt reveal confidential data.\n",
        "  * Don‚Äôt generate harmful content.\n",
        "  * Limit what actions can be taken (e.g., can‚Äôt charge more than \\$100 without approval).\n",
        "\n",
        "\n",
        "‚úÖ **Key Idea**: **Guardrails = rules to keep agents safe and controlled.**\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 4. **Sessions**\n",
        "\n",
        "* **What it is**:\n",
        "  A **Session** is like a chat history between a user and an agent.\n",
        "\n",
        "  * It remembers past interactions.\n",
        "  * Keeps context across multiple turns.\n",
        "\n",
        "* **Analogy**: Imagine you‚Äôre chatting with a friend. If you say:\n",
        "\n",
        "  * \"What‚Äôs the capital of France?\" ‚Üí they answer \"Paris\".\n",
        "  * Then you say: \"Book me a flight there.\" ‚Üí they remember \"there = Paris\".\n",
        "\n",
        "* Without sessions, the agent forgets the past.\n",
        "\n",
        "**Code Example:**\n",
        "\n",
        "‚úÖ **Key Idea**: **Sessions = memory of a conversation, so the agent doesn‚Äôt forget context.**\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# Quick Recap\n",
        "\n",
        "* **Agents** ‚Üí The AI workers (brains).\n",
        "* **Handoffs** ‚Üí Passing tasks between agents (teamwork).\n",
        "* **Guardrails** ‚Üí Safety rules & boundaries.\n",
        "* **Sessions** ‚Üí Memory of conversations (context).\n",
        "\n",
        "Together, these primitives = key foundation for building **safe, smart, multi-step AI systems**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfvGWfuEuf3u"
      },
      "source": [
        "## **Chat Completions and Responses API**\n",
        "\n",
        "## **1. Chat Completions API (older style)**\n",
        "\n",
        "* **What it is:** The API that powered models like `gpt-3.5-turbo` and `gpt-4`.\n",
        "* **How it works:** You send a conversation as a list of **messages** (`system`, `user`, `assistant`), and the model replies with a completion.\n",
        "* **Example:**\n",
        "\n",
        "  ```python\n",
        "  from openai import OpenAI\n",
        "  client = OpenAI()\n",
        "\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"gpt-4\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "          {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  print(completion.choices[0].message[\"content\"])\n",
        "  ```\n",
        "* **Limitations:**\n",
        "\n",
        "  * Only text input/output.\n",
        "  * Needed role-based messages (`system`, `user`, `assistant`).\n",
        "  * Couldn‚Äôt easily combine different output types (text + image + structured JSON).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **2. Responses API (newer, unified API)**\n",
        "\n",
        "* **What it is:** A **more flexible replacement** for Chat Completions.\n",
        "* **How it works:** Instead of only \"chat messages,\" you can define **input/output modalities** (text, audio, image, structured data). It‚Äôs meant to unify different model capabilities.\n",
        "* **Example:**\n",
        "\n",
        "  ```python\n",
        "  from openai import OpenAI\n",
        "  client = OpenAI()\n",
        "\n",
        "  response = client.responses.create(\n",
        "      model=\"gpt-4.1-mini\",\n",
        "      input=[{\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}]\n",
        "  )\n",
        "\n",
        "  print(response.output_text)\n",
        "  ```\n",
        "* **Key Advantages:**\n",
        "\n",
        "  * Supports **multi-turn chat** like before, but also multimodal inputs (text, images, audio).\n",
        "  * Output can include **multiple modalities** (e.g., text + image + tool call).\n",
        "  * Structured output is easier (JSON, function calls).\n",
        "  * Simplified: no need for \"chat.completions vs completions vs embeddings\" ‚Äî all unified.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Main Differences**\n",
        "\n",
        "| Feature        | Chat Completions API | Responses API                                        |\n",
        "| -------------- | -------------------- | ---------------------------------------------------- |\n",
        "| Statefulness   | Stateless; developer manages history | stateful; server manages history |\n",
        "| Built0in Tools   | Limited(text, vision, function calling            | Web search, file search, computer use, etc            |\n",
        "| Complexity | Simpler, for quick reponses                 | More complex, for agentic workflow                                              |\n",
        "| Use Case | Basic chatbots, single-turn tasks  | Advnaced agents, multi-turn task                 |\n",
        "| Latency | Lower, faster for simple queries  | Hiher, but optimized for complex tasks                |\n",
        "| Token Management | Manual trunction and context management  | Automatic history and token management                 |\n",
        "| Future Support | Infinite support for non-tool-dependent-model  | Represents OpenAI's future for agentic apps                 |\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **In short:**\n",
        "\n",
        "* **Chat Completions** = legacy, text-only chat interface.\n",
        "* **Responses API** = new unified API that can handle **text, images, audio, structured outputs, and tool calls** ‚Äî everything in one place.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBdAfeEI9_hW"
      },
      "source": [
        "## <font color= orange>**OpenRouter VS Lite LLM**</font>\n",
        "\n",
        "### **What is OpenRouter?**\n",
        "\n",
        "* Think of **OpenRouter** as the **‚Äúapp store for AI models‚Äù**.\n",
        "* It lets you **access multiple LLMs (Large Language Models)** through a single API, without needing to sign up with each provider separately.\n",
        "* You get **one API key**, but can use models from **OpenAI, Anthropic (Claude), Google (Gemini), Mistral, Meta (LLaMA), etc.**\n",
        "* It also provides features like **model routing** (choosing the best model), **fallbacks** (if one fails), and sometimes **cheaper or faster alternatives**.\n",
        "\n",
        "üëâ Example: Instead of juggling 5 different accounts (OpenAI, Anthropic, Hugging Face, etc.), you just use OpenRouter and say:\n",
        "\n",
        "```json\n",
        "{ \"model\": \"anthropic/claude-3.5\", \"input\": \"Summarize this text...\" }\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Best Usage Fit for OpenRouter**\n",
        "\n",
        "* **Experimenters & Students** ‚Üí Easily try out many models without managing separate accounts.\n",
        "* **Startups** ‚Üí Want to avoid vendor lock-in and switch between models based on cost/quality.\n",
        "* **Production apps** ‚Üí Want automatic **failover** (if one model is down, use another).\n",
        "* **Cost optimization** ‚Üí Pick cheaper models for easy tasks, expensive ones for harder reasoning.\n",
        "\n",
        "üí° Analogy: OpenRouter is like **Spotify for AI models** ‚Äî instead of buying from each record label, you stream all in one place.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **What is LiteLLM?**\n",
        "\n",
        "* **LiteLLM** is an **open-source library** that acts as a **translator / universal adapter** for LLM APIs.\n",
        "* It provides a **single unified API** (`litellm.completion()`) but works with **over 100+ LLM providers** (OpenAI, Anthropic, Cohere, Hugging Face, OpenRouter, even local models).\n",
        "* You can swap models by **just changing the model name**, not rewriting your whole codebase.\n",
        "\n",
        "üëâ Example with LiteLLM:\n",
        "\n",
        "```python\n",
        "from litellm import completion\n",
        "\n",
        "response = completion(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello world\"}]\n",
        ")\n",
        "print(response)\n",
        "```\n",
        "\n",
        "Switch to Anthropic with zero code changes:\n",
        "\n",
        "```python\n",
        "response = completion(\n",
        "    model=\"claude-3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello world\"}]\n",
        ")\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Best Usage Fit for LiteLLM**\n",
        "\n",
        "* **Developers** ‚Üí Want to write code **once** and plug into **any LLM provider** easily.\n",
        "* **Teams building agents** ‚Üí Agents may need to call different models (fast vs reasoning-heavy). LiteLLM makes switching trivial.\n",
        "* **Companies** ‚Üí Want to run their own ‚Äúmini OpenRouter‚Äù internally (LiteLLM can act as a proxy server).\n",
        "* **Tool builders** ‚Üí If you‚Äôre building an app like ChatGPT clone, LiteLLM helps integrate multiple providers without headaches.\n",
        "\n",
        "üí° Analogy: LiteLLM is like a **universal phone charger** ‚Äî one port (API) to connect with many different ‚Äúphones‚Äù (LLMs).\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Difference & When to Use**\n",
        "\n",
        "| Feature        | **OpenRouter API**                | **LiteLLM API**                                     |\n",
        "| -------------- | ------------------------------------------------ | ------------------------------------------------------------------- |\n",
        "| **What it is** | API marketplace for models                       | Open-source universal LLM adapter                                   |\n",
        "| **Setup**      | Just get an API key, no infra needed             | Install library / run server                                        |\n",
        "| **Models**     | Hosted selection of models                       | Any model (OpenAI, Anthropic, HuggingFace, OpenRouter, local, etc.) |\n",
        "| **Routing**    | Built-in (choose best/cheapest model)            | You control routing logic                                           |\n",
        "| **Best fit**   | Non-technical users, startups, quick prototyping | Developers, infra teams, production systems needing control         |\n",
        "| **Lock-in**    | Tied to OpenRouter service                       | No lock-in, self-hostable, open-source                              |\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "# **Putting It Together**\n",
        "\n",
        "üëâ **If you‚Äôre a beginner or startup:**\n",
        "Use **OpenRouter** ‚Äî easy, plug-and-play, try many models without the hassle.\n",
        "\n",
        "üëâ **If you‚Äôre a developer building serious systems:**\n",
        "Use **LiteLLM** ‚Äî keep full control, run locally or in your infra, and even plug OpenRouter **inside LiteLLM** if you want!\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "‚ö° Pro tip: Many production apps **combine both**:\n",
        "\n",
        "* LiteLLM as the **adapter layer**\n",
        "* OpenRouter as **one of the providers** inside LiteLLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqYcYX0RE3eg"
      },
      "source": [
        "## <font color= orange>**Global, Run & Agent Level Config**</font>\n",
        "\n",
        "### **Global Config**\n",
        "\n",
        "* Applies to the **entire SDK session or application**.\n",
        "* Think of it as ‚Äúdefault environment settings‚Äù for everything.\n",
        "* Example: API keys, default LLM provider, logging, safety filters.\n",
        "\n",
        "üí° Analogy: **School rules** ‚Üí apply to all students unless a teacher overrides them.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Run Config**\n",
        "\n",
        "* Applies to **a single agent run (conversation or task execution)**.\n",
        "* Lets you override some global defaults for that run.\n",
        "* Example: Change model or temperature **just for one run** without affecting others.\n",
        "\n",
        "üí° Analogy: **Class session rules** ‚Üí teacher says ‚Äútoday we do group work‚Äù even though school normally does lectures.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Agent Config**\n",
        "\n",
        "* Applies to a **specific agent instance**.\n",
        "* Defines the agent‚Äôs **role, tools, memory, and default model**.\n",
        "* Example: FinanceAgent always uses `gpt-4` + stock API; TravelAgent uses `gpt-4-mini` + booking API.\n",
        "\n",
        "üí° Analogy: **Each teacher‚Äôs teaching style** ‚Üí math teacher vs history teacher.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# Summary Table\n",
        "\n",
        "| Config Level  | Scope              | Best Usage                             | Example                                     |\n",
        "| ------------- | ------------------ | -------------------------------------- | ------------------------------------------- |\n",
        "| **Global** üåç | Whole application  | Default provider/settings for everyone | Default = GPT-4-mini                        |\n",
        "| **Run** üèÉ    | One execution      | Temporary override for special task    | Use Claude just for one run                 |\n",
        "| **Agent** ü§ñ  | One agent instance | Specialized role & provider            | Math tutor = GPT-4, Travel planner = Claude |\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Pro Tips**\n",
        "\n",
        "* Use **Global Config** for API keys, logging, default provider.\n",
        "* Use **Agent Config** for specializations (different models for different tasks).\n",
        "* Use **Run Config** when you need **flexibility inside one conversation** (e.g., fallback model, cheaper model for summarization).\n",
        "* You can **nest them**: Global ‚Üí Agent ‚Üí Run (Run always wins if conflicting).\n",
        "\n",
        "---\n",
        "\n",
        "**Example Combo:**\n",
        "\n",
        "* Global = GPT-4-mini\n",
        "* Travel Agent = Claude\n",
        "* Inside one run of Travel Agent = temporarily switch to GPT-4-Turbo for hard reasoning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak-wVzzpHhPS"
      },
      "source": [
        "\n",
        "### **How to Configure LLM Providers**\n",
        "\n",
        "### **Global Level (System-Wide Default)**\n",
        "\n",
        "Best when: you want **all agents to use the same provider by default**.\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "# Global config: all agents will default to GPT-4.1-mini\n",
        "client = OpenAI(\n",
        "    api_key=\"GLOBAL_API_KEY\",\n",
        "    default_model=\"gpt-4.1-mini\"\n",
        ")\n",
        "```\n",
        "\n",
        "‚úÖ Every agent/run will use `gpt-4.1-mini` unless explicitly changed.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Run Level (Task-Specific Override)**\n",
        "\n",
        "Best when: you want to **switch providers for one task**.\n",
        "\n",
        "```python\n",
        "\n",
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
        "from agents.run import RunConfig\n",
        "\n",
        "#Reference: https://ai.google.dev/gemini-api/docs/openai\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=external_client,\n",
        "    tracing_disabled=True\n",
        ")\n",
        "\n",
        "agent: Agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n",
        "\n",
        "result = Runner.run_sync(agent, \"Hello, how are you.\", run_config=config)\n",
        "\n",
        "print(result.final_output)\n",
        "```\n",
        "\n",
        "‚úÖ Global default = GPT-4.1-mini\n",
        "‚úÖ This run = Claude 3.5\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Agent Level (Per-Agent Personality/Tools)**\n",
        "\n",
        "Best when: you want **different agents specialized with different providers**.\n",
        "\n",
        "```python\n",
        "from openai.agents import run\n",
        "\n",
        "# Agent 1: Math tutor (uses OpenAI GPT-4)\n",
        "math_agent = {\n",
        "    \"instructions\": \"You are a math tutor.\",\n",
        "    \"model\": \"gpt-4\"\n",
        "}\n",
        "\n",
        "# Agent 2: Travel planner (uses Anthropic Claude)\n",
        "travel_agent = {\n",
        "    \"instructions\": \"You are a travel planner.\",\n",
        "    \"model\": \"anthropic/claude-3.5\"\n",
        "}\n",
        "\n",
        "# Run them\n",
        "res1 = run(client=client, **math_agent, input=\"What is 23 * 47?\")\n",
        "res2 = run(client=client, **travel_agent, input=\"Find me a flight to Tokyo\")\n",
        "\n",
        "print(\"Math Agent:\", res1.output)\n",
        "print(\"Travel Agent:\", res2.output)\n",
        "```\n",
        "\n",
        "‚úÖ Both agents share the same `client` (global config like API key, logging).\n",
        "\n",
        "‚úÖ But each has **its own LLM provider + persona**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_VN00kqLU_J"
      },
      "source": [
        "# **Chat Completions vs Reponses API**\n",
        "## **1. Chat Completions API**\n",
        "\n",
        "* **Purpose** ‚Üí Power conversational AI (like ChatGPT).\n",
        "* **Input** ‚Üí A *list of messages* (`system`, `user`, `assistant`, etc.) that give context to the conversation.\n",
        "* **Output** ‚Üí A model-generated **chat message** (usually the assistant‚Äôs reply).\n",
        "\n",
        "**Best when:**\n",
        "\n",
        "* You want natural back-and-forth conversation.\n",
        "* You‚Äôre building **agents, chatbots, copilots, or interactive systems**.\n",
        "\n",
        "üìå **Example:**\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain recursion in simple terms.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message[\"content\"])\n",
        "```\n",
        "\n",
        "Output ‚Üí `\"Recursion is when a function calls itself to solve a smaller version of the problem...\"`\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **2. Responses API**\n",
        "\n",
        "* **Purpose** ‚Üí A **newer, unified API** that goes beyond chat.\n",
        "* **Input** ‚Üí Similar to chat (messages), but more **flexible**:\n",
        "\n",
        "  * Can handle **text, images, audio, video** all in one request.\n",
        "  * Can run **structured outputs** (JSON mode, tools, function calling).\n",
        "* **Output** ‚Üí Rich responses: text, function calls, tool outputs, even multi-modal data.\n",
        "\n",
        "‚úÖ **Best when:**\n",
        "\n",
        "* You need **multi-modal reasoning** (text + images + audio).\n",
        "* You want **fine-grained control** over responses.\n",
        "* You‚Äôre building **agentic systems** where the model not only chats but also *calls functions, routes, or generates structured data*.\n",
        "\n",
        "üìå **Example:**\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    input=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a JSON-only assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Give me a list of 3 fruit in JSON.\"}\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ")\n",
        "\n",
        "print(response.output_parsed)\n",
        "```\n",
        "\n",
        "Output ‚Üí\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"fruits\": [\"apple\", \"banana\", \"mango\"]\n",
        "}\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# **Key Differences**\n",
        "\n",
        "| Feature                  | Chat Completions API üó®Ô∏è    | Responses API üß©                              |\n",
        "| ------------------------ | --------------------------- | --------------------------------------------- |\n",
        "| Input format             | Chat messages only          | Messages + multi-modal input                  |\n",
        "| Output                   | Text messages               | Text, JSON, tool calls, structured data       |\n",
        "| Multi-modal (text+image) | ‚ùå No                        | ‚úÖ Yes                                         |\n",
        "| Function calling/tools   | Limited                     | First-class support                           |\n",
        "| Recommended for          | Conversational AI, chatbots | Agents, complex workflows, structured outputs |\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **TL;DR**\n",
        "\n",
        "* **Chat Completions** = best for **straight chatbots**.\n",
        "* **Responses API** = the **next-gen unified API**, great for **agents, multi-modal, structured outputs**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(**System prompt, User prompt, Tool schema, Tool output**) are the **practical 4 categories** of content that actually go into an **LLM call inside an Agent Loop**. Let‚Äôs break them down carefully so you don‚Äôt get confused:\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# 1. **System Prompt (Instructions / Persona)**\n",
        "\n",
        "* Defines **who the agent is** and **how it should behave**.\n",
        "* Given once at the start of the conversation (but always included in every LLM call).\n",
        "* Example:\n",
        "\n",
        "  ```\n",
        "  \"You are a helpful assistant that responds only in haikus.\"\n",
        "  ```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# 2. **User Prompt (Input / Conversation History)**\n",
        "\n",
        "* The **user‚Äôs message** or the latest query in the loop.\n",
        "* Also includes **past turns** (conversation memory) so the model has context.\n",
        "* Example:\n",
        "\n",
        "  ```\n",
        "  User: \"What‚Äôs the weather in Karachi?\"\n",
        "  ```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# 3. **Tool Schema (Functions the LLM Can Call)**\n",
        "\n",
        "* The LLM is told about the **tools available** (names, descriptions, parameters).\n",
        "* This allows the model to *decide if it should call a tool instead of answering*.\n",
        "* Example schema sent to the LLM:\n",
        "\n",
        "  ```json\n",
        "  {\n",
        "    \"name\": \"get_weather\",\n",
        "    \"description\": \"Fetches current weather by city.\",\n",
        "    \"parameters\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"city\": {\"type\": \"string\"}\n",
        "      },\n",
        "      \"required\": [\"city\"]\n",
        "    }\n",
        "  }\n",
        "  ```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# 4. **Tool Output (Results of Previous Tool Calls)**\n",
        "\n",
        "* If the agent already called a tool, the tool‚Äôs result is passed back **as part of the conversation history**.\n",
        "* This ensures the LLM can use the result in the next reasoning step.\n",
        "* Example:\n",
        "\n",
        "  ```\n",
        "  Tool[get_weather]: \"The weather in Karachi is Sunny, 30¬∞C.\"\n",
        "  ```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# **How These Work Together in the Agent Loop**\n",
        "\n",
        "üëâ A **single LLM call** looks like this bundle:\n",
        "\n",
        "```\n",
        "System Prompt   ‚Üí defines agent persona\n",
        "User Prompt     ‚Üí latest input + conversation history\n",
        "Tool Schema     ‚Üí functions/tools available\n",
        "Tool Output     ‚Üí any previous tool results\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Example Flow**\n",
        "\n",
        "**User:** ‚ÄúWhat‚Äôs the square root of today‚Äôs temperature in Karachi?‚Äù\n",
        "\n",
        "1. **System Prompt** ‚Üí \"You are a helpful math + weather assistant.\"\n",
        "2. **User Prompt** ‚Üí \"What‚Äôs the square root of today‚Äôs temperature in Karachi?\"\n",
        "3. **Tool Schema** ‚Üí (weather API, math function API)\n",
        "4. **Tool Output** ‚Üí (none yet, first call)\n",
        "\n",
        "LLM decides: *\"Call get\\_weather(city=Karachi)\"*\n",
        "‚Üí SDK executes it ‚Üí result = \"30¬∞C\"\n",
        "\n",
        "Next loop:\n",
        "\n",
        "1. System prompt (same)\n",
        "2. User prompt (conversation so far)\n",
        "3. Tool schema (same)\n",
        "4. Tool output = \"30¬∞C\"\n",
        "\n",
        "LLM now says: *\"Call math.sqrt(30)\"*\n",
        "‚Üí result = 5.477\n",
        "\n",
        "Final loop: LLM outputs final answer: *\"The square root of today‚Äôs temperature (30) is about 5.48.\"*\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
