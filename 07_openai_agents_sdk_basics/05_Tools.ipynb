{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### [Open in Colab][1]\n",
        "\n",
        "[1]:https://colab.research.google.com/drive/1wr4I52iSROtHEUpViOtKCR2S37jBxvb9?usp=sharing\n",
        "\n"
      ],
      "metadata": {
        "id": "UnP0jeG40BXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Agent Tools**"
      ],
      "metadata": {
        "id": "uMG0Hftg_q8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What are Tools in Agents?**\n",
        "\n",
        "In Agentic AI:\n",
        "\n",
        "* **LLMs can‚Äôt actually ‚Äúdo‚Äù things** on their own (like fetch real data, run code, or call APIs).\n",
        "* **Tools** are how you let an agent *extend its powers* by calling **functions you define in Python** (or external APIs).\n",
        "* The agent decides *when* and *how* to call a tool, based on the user‚Äôs input.\n",
        "\n",
        "Think of tools as:\n",
        "üëâ *‚Äúskills or superpowers that an agent can use when it can‚Äôt just rely on text reasoning.‚Äù*\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# **How Tools Work in the Agent Loop**\n",
        "\n",
        "1. **User gives input** ‚Üí ‚ÄúWhat‚Äôs the weather in Karachi?‚Äù\n",
        "2. Agent tries to solve it.\n",
        "3. If the model realizes it needs external info, it says:\n",
        "\n",
        "   > *‚ÄúI should call the `get_weather` tool with city=Karachi.‚Äù*\n",
        "4. The SDK executes your Python function (`get_weather(\"Karachi\")`).\n",
        "5. The tool‚Äôs output is fed back to the agent.\n",
        "6. The agent produces the **final answer** to the user.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# **Defining Tools**\n",
        "\n",
        "The SDK makes this **super simple**. You use the `@function_tool` decorator.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "from agents import function_tool\n",
        "\n",
        "@function_tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Fetches weather for a given city.\"\"\"\n",
        "    return f\"The weather in {city} is sunny with 30¬∞C.\"\n",
        "```\n",
        "\n",
        "* The decorator does the heavy lifting:\n",
        "\n",
        "  * It tells the agent **what the function does** (docstring).\n",
        "  * It registers the function signature (`city: str`).\n",
        "  * It makes the tool available for the agent‚Äôs reasoning loop.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# **Adding Tools to an Agent**\n",
        "\n",
        "When you create the agent, just pass a list of tools:\n",
        "\n",
        "```python\n",
        "from agents import Agent\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"WeatherBot\",\n",
        "    instructions=\"You answer weather-related questions.\",\n",
        "    tools=[get_weather],   # ‚úÖ now agent can call this tool\n",
        ")\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# **Why Tools are Powerful**\n",
        "\n",
        "* **APIs** ‚Üí connect to weather, stock prices, databases.\n",
        "* **Math/logic** ‚Üí do calculations the LLM can‚Äôt.\n",
        "* **Custom workflows** ‚Üí trigger emails, run SQL queries, call other services.\n",
        "* **Multi-agent collaboration** ‚Üí one agent can call another as a ‚Äútool.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **In short:**\n",
        "Tools let you go *beyond text*.\n",
        "They turn an LLM from a *talker* ‚Üí into a *doer*.\n",
        "\n"
      ],
      "metadata": {
        "id": "PWzvQ46BgxjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq openai-agents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ulYpwLCnZUBX",
        "outputId": "c57372dd-0157-4eb3-e1b1-30bc0fc2514a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-agents\n",
            "  Downloading openai_agents-0.3.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting griffe<2,>=1.5.6 (from openai-agents)\n",
            "  Downloading griffe-1.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: mcp<2,>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (1.13.1)\n",
            "Collecting openai<2,>=1.107.1 (from openai-agents)\n",
            "  Downloading openai-1.107.3-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2.10 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (2.11.7)\n",
            "Requirement already satisfied: requests<3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (2.32.4)\n",
            "Collecting types-requests<3,>=2.0 (from openai-agents)\n",
            "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (4.15.0)\n",
            "Collecting colorama>=0.4 (from griffe<2,>=1.5.6->openai-agents)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (4.10.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.4.1)\n",
            "Requirement already satisfied: httpx>=0.27.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.28.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (3.0.2)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.47.3)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.35.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.107.1->openai-agents) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.107.1->openai-agents) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.107.1->openai-agents) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.107.1->openai-agents) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.10->openai-agents) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.10->openai-agents) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.10->openai-agents) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.5.2->mcp<2,>=1.11.0->openai-agents) (1.1.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp<2,>=1.11.0->openai-agents) (8.2.1)\n",
            "Downloading openai_agents-0.3.0-py3-none-any.whl (185 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading griffe-1.14.0-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.107.3-py3-none-any.whl (947 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m947.6/947.6 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: types-requests, colorama, griffe, openai, openai-agents\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.106.1\n",
            "    Uninstalling openai-1.106.1:\n",
            "      Successfully uninstalled openai-1.106.1\n",
            "Successfully installed colorama-0.4.6 griffe-1.14.0 openai-1.107.3 openai-agents-0.3.0 types-requests-2.32.4.20250913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "o40kr-Z3ydGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import os\n",
        "\n",
        "from agents import  Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, function_tool, set_tracing_disabled\n",
        "from google.colab import userdata\n",
        "\n",
        "# Debug Logging\n",
        "# from agents import enable_verbose_stdout_logging\n",
        "# enable_verbose_stdout_logging()\n",
        "\n",
        "# Disable tracing globally\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "# Setup key\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "MODEL= \"gemini-2.5-flash\"\n",
        "\n",
        "external_client= AsyncOpenAI(\n",
        "    api_key= GEMINI_API_KEY,\n",
        "    base_url= \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "model= OpenAIChatCompletionsModel(\n",
        "    model= MODEL,\n",
        "    openai_client= external_client,\n",
        ")\n",
        "\n",
        "# Example tool\n",
        "@function_tool\n",
        "def get_weather(city: str) -> str:\n",
        "  print(\"[DEBUG] getting the weather data\")\n",
        "  return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "# Agent\n",
        "agent = Agent(\n",
        "    name= \"Assistant\",\n",
        "    instructions= \"You only respond in haikus.\",\n",
        "    model= model,\n",
        "    tools= [get_weather],\n",
        ")\n",
        "\n",
        "async def main():\n",
        "  result = await Runner.run(starting_agent= agent, input=\"What is current weather in Karachi.\")\n",
        "  print(result.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2DIYPpdZZRX",
        "outputId": "2d9205ee-ca9d-4644-e7d2-58c3d6c932f0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating trace Agent workflow\n",
            "Tracing is disabled. Not creating trace Agent workflow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating trace Agent workflow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting current trace: no-op\n",
            "Setting current trace: no-op\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Setting current trace: no-op\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x7c2911f66c60>\n",
            "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x7c2911f66c60>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x7c2911f66c60>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running agent Assistant (turn 1)\n",
            "Running agent Assistant (turn 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Running agent Assistant (turn 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7c2912c13110>\n",
            "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7c2912c13110>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7c2912c13110>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling LLM\n",
            "Calling LLM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Calling LLM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received model response\n",
            "Received model response\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Received model response\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.FunctionSpanData object at 0x7c2912c471b0>\n",
            "Tracing is disabled. Not creating span <agents.tracing.span_data.FunctionSpanData object at 0x7c2912c471b0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.FunctionSpanData object at 0x7c2912c471b0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invoking tool get_weather\n",
            "Invoking tool get_weather\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Invoking tool get_weather\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] getting the weather data\n",
            "Tool get_weather completed.\n",
            "Tool get_weather completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tool get_weather completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running agent Assistant (turn 2)\n",
            "Running agent Assistant (turn 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Running agent Assistant (turn 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7c2911fd98b0>\n",
            "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7c2911fd98b0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x7c2911fd98b0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling LLM\n",
            "Calling LLM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Calling LLM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received model response\n",
            "Received model response\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Received model response\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resetting current trace\n",
            "Resetting current trace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai.agents:Resetting current trace\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun shines bright today,\n",
            "Karachi's warmth fills the air,\n",
            "A pleasant forecast.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function Tools**\n",
        "\n",
        "### **What They Are**\n",
        "\n",
        "* **Function tools** let you connect an LLM (like GPT-4 or o-series models) to **external functions, APIs, or custom logic**.\n",
        "* The model doesn‚Äôt just generate text ‚Äî it can say *‚ÄúI need to call this tool with these arguments‚Äù* ‚Üí then your code runs that function ‚Üí and the result is fed back into the model.\n",
        "\n",
        "üëâ This is how agents can:\n",
        "\n",
        "* Fetch real-time data\n",
        "* Query databases\n",
        "* Run calculations\n",
        "* Call APIs\n",
        "* Automate workflows\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **How They Work (Step by Step)**\n",
        "\n",
        "1. **Define a function tool**\n",
        "\n",
        "   * You describe the tool: its name, description, and parameters.\n",
        "\n",
        "2. **Model decides when to use it**\n",
        "\n",
        "   * Based on the user‚Äôs request, the model can output a tool call (structured JSON).\n",
        "\n",
        "3. **Agent executes the tool**\n",
        "\n",
        "   * Your SDK agent receives the tool call ‚Üí runs the Python function (or other logic).\n",
        "\n",
        "4. **Results go back to the model**\n",
        "\n",
        "   * The tool‚Äôs return value is passed back into the model, which then produces the final answer.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Anatomy of a Tool**\n",
        "\n",
        "A function tool usually has:\n",
        "\n",
        "* **Name**: unique identifier (`\"get_weather\"`).\n",
        "* **Description**: natural language explanation of what it does.\n",
        "* **Parameters**: JSON schema defining inputs (types, required fields).\n",
        "* **Implementation**: actual Python function code.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Example in Python (Agents SDK)**\n",
        "\n",
        "```python\n",
        "from openai import tool\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# 1. Define input schema\n",
        "class WeatherRequest(BaseModel):\n",
        "    location: str\n",
        "    unit: str\n",
        "\n",
        "# 2. Define the function tool\n",
        "@tool\n",
        "def get_weather(data: WeatherRequest) -> str:\n",
        "    \"\"\"Get the current weather for a location.\"\"\"\n",
        "    # Imagine calling a real weather API here\n",
        "    if data.location.lower() == \"london\":\n",
        "        return f\"Weather in {data.location}: 15¬∞C and cloudy.\"\n",
        "    return f\"Weather in {data.location}: 28¬∞C and sunny.\"\n",
        "\n",
        "# 3. Create an agent with the tool\n",
        "from openai import Agent\n",
        "\n",
        "agent = Agent(\n",
        "    model=\"gpt-4.1\",\n",
        "    tools=[get_weather]  # register the function tool\n",
        ")\n",
        "\n",
        "# 4. Ask the agent\n",
        "response = agent.run(\"What's the weather in London in Celsius?\")\n",
        "print(response.output_text)\n",
        "```\n",
        "\n",
        "**What happens internally**:\n",
        "\n",
        "1. Model parses user request ‚Üí ‚ÄúI need weather in London.‚Äù\n",
        "2. Model outputs a tool call ‚Üí `get_weather({\"location\": \"London\", \"unit\": \"C\"})`.\n",
        "3. SDK runs `get_weather()` in Python.\n",
        "4. Result goes back ‚Üí ‚ÄúWeather in London: 15¬∞C and cloudy.‚Äù\n",
        "5. Agent returns final answer.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Benefits**\n",
        "\n",
        "* **Dynamic capabilities** ‚Üí agent can fetch fresh data (not limited to training cutoff).\n",
        "* **Structured inputs** ‚Üí model can only call with valid JSON schema (avoids messy parsing).\n",
        "* **Separation of concerns** ‚Üí model handles reasoning, your code handles execution.\n",
        "* **Scalability** ‚Üí you can chain multiple tools (database, APIs, calculators, etc.).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## **Best Practices**\n",
        "\n",
        "* Keep tool descriptions **clear and human-readable** ‚Üí helps the model know when to use them.\n",
        "* Define **strict schemas** with Pydantic ‚Üí avoids invalid inputs.\n",
        "* Use **multiple tools** for modularity (e.g., `search_api`, `calculator`, `database_query`).\n",
        "* Monitor tool calls ‚Üí log inputs/outputs for debugging.\n",
        "* If you want **parallel tool calls**, enable `parallel_tool_calls` in `ModelSettings`.\n",
        "\n",
        "---\n",
        "\n",
        "üëâ In short:\n",
        "**Function tools = the bridge between LLM reasoning and real-world actions.**\n",
        "They turn your agent from a *chatbot* into a *problem-solver*.\n",
        "\n"
      ],
      "metadata": {
        "id": "1mTcRKhDOfNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Python Docstrings**\n",
        "Python docstrings are string literals that show information regarding Python functions, classes, methods, and modules, allowing them to be properly documented.\n",
        "\n",
        "**1. Google Style**\n",
        "- Simple, readable, indentation-based.\n",
        "- Uses sections like `Args`, `Returns`, `Raises`.\n",
        "- Example:\n",
        "  ```python\n",
        "  def add(a: int, b: int) -> int:\n",
        "      \"\"\"Add two numbers.\n",
        "  \n",
        "      Args:\n",
        "          a (int): First number.\n",
        "          b (int): Second number.\n",
        "  \n",
        "      Returns:\n",
        "          int: Sum of a and b.\n",
        "      \"\"\"\n",
        "  ```\n",
        "- Best for: Beginners, small projects.\n",
        "\n",
        "**2. NumPy Style**\n",
        "- Detailed, structured, dash-separated sections.\n",
        "- Uses `Parameters`, `Returns`, `Examples`, etc.\n",
        "- Example:\n",
        "  ```python\n",
        "  def multiply(a: float, b: float) -> float:\n",
        "      \"\"\"Multiply two numbers.\n",
        "  \n",
        "      Parameters\n",
        "      ----------\n",
        "      a : float\n",
        "          First number.\n",
        "      b : float\n",
        "          Second number.\n",
        "  \n",
        "      Returns\n",
        "      -------\n",
        "      float\n",
        "          Product of a and b.\n",
        "      \"\"\"\n",
        "  ```\n",
        "- Best for: Scientific projects, data science.\n",
        "\n",
        "**3. Sphinx (reStructuredText) Style**\n",
        "- Formal, uses reST markup (`:param`, `:return:`).\n",
        "- Supports cross-referencing, complex formatting.\n",
        "- Example:\n",
        "  ```python\n",
        "  def divide(a: float, b: float) -> float:\n",
        "      \"\"\"Divide two numbers.\n",
        "  \n",
        "      :param a: Numerator.\n",
        "      :type a: float\n",
        "      :param b: Denominator.\n",
        "      :type b: float\n",
        "      :return: Quotient of a and b.\n",
        "      :rtype: float\n",
        "      \"\"\"\n",
        "  ```\n",
        "- Best for: Large projects, professional docs.\n",
        "\n",
        "**Comparison**\n",
        "- **Google**: Simple, beginner-friendly.\n",
        "- **NumPy**: Detailed, great for examples.\n",
        "- **Sphinx**: Formal, ideal for Sphinx-generated docs.\n",
        "- **Tip**: Stick to one style per project, use type hints, and include examples."
      ],
      "metadata": {
        "id": "979PHrt6RLlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **`function_tool` Explained**\n",
        "\n",
        "```python\n",
        "def function_tool(\n",
        "    func: ToolFunction[...],\n",
        "    *,\n",
        "    name_override: str | None = None,\n",
        "    description_override: str | None = None,\n",
        "    docstring_style: DocstringStyle | None = None,\n",
        "    use_docstring_info: bool = True,\n",
        "    failure_error_function: ToolErrorFunction | None = None,\n",
        "    strict_mode: bool = True,\n",
        "    is_enabled: bool | Callable[[RunContextWrapper[Any], AgentBase], MaybeAwaitable[bool]] = True,\n",
        ") -> FunctionTool:\n",
        "    ...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Purpose**\n",
        "\n",
        "* Converts a **Python function** into a **FunctionTool** object.\n",
        "* FunctionTools are what the **agent advertises** to the LLM (so the model knows: ‚Äúthis tool exists, here‚Äôs its schema, here‚Äôs when you can use it‚Äù).\n",
        "* Handles **naming, description, docstring parsing, error handling, and availability logic**.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Parameter-by-Parameter**\n",
        "\n",
        "### 1. `func: ToolFunction[...]`\n",
        "\n",
        "* The **Python function** you want to expose as a tool.\n",
        "* Must have **type annotations** (or Pydantic models) so the SDK can generate the JSON schema.\n",
        "* Example:\n",
        "\n",
        "  ```python\n",
        "  def get_weather(city: str, unit: str) -> str:\n",
        "      ...\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `name_override: str | None`\n",
        "\n",
        "* Lets you override the **tool‚Äôs name**.\n",
        "* By default, the function‚Äôs Python name (`get_weather`) is used.\n",
        "* Use this if you want a different, cleaner, or API-friendly name.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `description_override: str | None`\n",
        "\n",
        "* Lets you override the **tool‚Äôs description**.\n",
        "* By default, the function‚Äôs **docstring** is used.\n",
        "* Useful if you want a more natural-language description for the model.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. `docstring_style: DocstringStyle | None`\n",
        "\n",
        "* Controls how the function‚Äôs **docstring is parsed** into tool metadata.\n",
        "* For example, Google-style vs NumPy-style docstrings.\n",
        "* This affects how parameters/descriptions are extracted.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. `use_docstring_info: bool`\n",
        "\n",
        "* Whether to **use the docstring** to auto-generate the tool‚Äôs description/parameter help.\n",
        "* `True` (default) ‚Üí docstrings are parsed.\n",
        "* `False` ‚Üí you must provide your own overrides.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. `failure_error_function: ToolErrorFunction | None`\n",
        "\n",
        "* A custom **error handler**.\n",
        "* If your tool function raises an exception, this handler decides **what error message** is returned to the model.\n",
        "* Lets you control failure behavior instead of crashing.\n",
        "\n",
        "\n",
        "```python\n",
        "def failure_error_function(\n",
        "    error: Exception,\n",
        "    run_context: RunContextWrapper[Any],\n",
        "    agent: AgentBase\n",
        ") -> str | dict\n",
        "```\n",
        "\n",
        "* `error`: the Python exception raised by your tool.\n",
        "* `run_context`: runtime info about the tool call.\n",
        "* `agent`: the agent that called the tool.\n",
        "* Return value ‚Üí sent back to the model as the tool‚Äôs response.\n",
        "\n",
        "### Example: Weather Tool with Error Handling\n",
        "\n",
        "```python\n",
        "from openai.tools import function_tool\n",
        "\n",
        "# --- Tool function ---\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get the weather for a city.\"\"\"\n",
        "    if city.lower() not in [\"london\", \"paris\"]:\n",
        "        raise ValueError(f\"Weather data for {city} is not available.\")\n",
        "    return f\"Weather in {city}: 20¬∞C and sunny.\"\n",
        "\n",
        "# --- Failure handler ---\n",
        "def weather_error_handler(error: Exception, run_context, agent) -> str:\n",
        "    # You can log the error or customize response\n",
        "    return f\"‚ùå Error: {str(error)}. Please try another city.\"\n",
        "\n",
        "# --- Wrap tool with error handler ---\n",
        "weather_tool = function_tool(\n",
        "    get_weather,\n",
        "    failure_error_function=weather_error_handler\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7. `strict_mode: bool`\n",
        "\n",
        "* Controls **schema strictness** for tool arguments.\n",
        "* `True` ‚Üí only arguments that match the schema are allowed (default, safer).\n",
        "* `False` ‚Üí looser parsing, may accept extra/unknown arguments.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. `is_enabled: bool | Callable[...]`\n",
        "\n",
        "* Determines **whether the tool is currently available**.\n",
        "* Can be:\n",
        "\n",
        "  * `True` ‚Üí always enabled (default).\n",
        "  * `False` ‚Üí disabled, agent won‚Äôt advertise it.\n",
        "  * A **callable** ‚Üí dynamically decide based on runtime context (`RunContextWrapper`, `AgentBase`).\n",
        "\n",
        "    * Example: Only enable a ‚Äúdatabase\\_query‚Äù tool if the user is authenticated.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Example Usage**\n",
        "\n",
        "```python\n",
        "from openai.tools import function_tool\n",
        "\n",
        "def get_weather(city: str, unit: str) -> str:\n",
        "    \"\"\"Get the weather for a city.\n",
        "    \n",
        "    Args:\n",
        "        city: The city to check.\n",
        "        unit: Unit system, either 'C' or 'F'.\n",
        "    \"\"\"\n",
        "    return f\"Weather in {city}: 20¬∞{unit}\"\n",
        "\n",
        "weather_tool = function_tool(\n",
        "    get_weather,\n",
        "    name_override=\"fetch_weather\",\n",
        "    description_override=\"Retrieve the current weather for a given city.\",\n",
        "    strict_mode=True\n",
        ")\n",
        "```\n",
        "\n",
        "**Result**:\n",
        "\n",
        "* The agent now has a `FunctionTool` called **`fetch_weather`**.\n",
        "* Description: ‚ÄúRetrieve the current weather for a given city.‚Äù\n",
        "* JSON schema is auto-generated from type hints.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Summary**\n",
        "\n",
        "* **`function_tool`** = wrapper that makes a normal Python function usable by an agent.\n",
        "* Handles:\n",
        "\n",
        "  * üè∑Ô∏è Naming (`name_override`)\n",
        "  * üìñ Description (`description_override`, `docstring_style`)\n",
        "  * ‚ö†Ô∏è Error handling (`failure_error_function`)\n",
        "  * ‚úÖ Strictness (`strict_mode`)\n",
        "  * üö¶ Availability (`is_enabled`)\n",
        "\n",
        "üëâ In short: it turns **Python functions ‚Üí LLM-callable tools**.\n",
        "\n"
      ],
      "metadata": {
        "id": "vvgNvwLlXiMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`FunctionTool` - Custom Tool Creation**\n",
        "\n",
        "## **What It Is**\n",
        "\n",
        "* `FunctionTool` is the **core class** that represents a function tool in the OpenAI Agents SDK.\n",
        "* While `@tool` and `function_tool` are **convenience wrappers**, you can directly use `FunctionTool` when you want **full control**.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Structure**\n",
        "\n",
        "A `FunctionTool` generally needs:\n",
        "\n",
        "1. **Name** ‚Üí Unique identifier for the tool.\n",
        "2. **Description** ‚Üí What the tool does (helps the model know when to use it).\n",
        "3. **Parameters schema** ‚Üí JSON schema of inputs (usually built from type hints).\n",
        "4. **Implementation** ‚Üí A Python callable that executes the tool logic.\n",
        "5. **Optional features** ‚Üí error handling, enabled/disabled logic, etc.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Example 1: Basic Custom Tool**\n",
        "\n",
        "```python\n",
        "from openai.types.tools import FunctionTool\n",
        "import json\n",
        "\n",
        "# Define the actual function\n",
        "def reverse_text(text: str) -> str:\n",
        "    \"\"\"Reverse a given string of text.\"\"\"\n",
        "    return text[::-1]\n",
        "\n",
        "# Define input schema manually (JSON schema format)\n",
        "parameters = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"text\": {\"type\": \"string\", \"description\": \"The text to reverse\"}\n",
        "    },\n",
        "    \"required\": [\"text\"]\n",
        "}\n",
        "\n",
        "# Create FunctionTool\n",
        "reverse_tool = FunctionTool(\n",
        "    name=\"reverse_text\",\n",
        "    description=\"Reverses the given text string.\",\n",
        "    parameters=parameters,\n",
        "    function=lambda args: reverse_text(**json.loads(args))\n",
        ")\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Usage**\n",
        "\n",
        "```python\n",
        "# Simulating a model call with JSON args\n",
        "input_args = '{\"text\": \"OpenAI\"}'\n",
        "result = reverse_tool.function(input_args)\n",
        "print(result)  # \"IAnepO\"\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Example 2: With Error Handling**\n",
        "\n",
        "```python\n",
        "def safe_divide(a: float, b: float) -> float:\n",
        "    \"\"\"Divide two numbers safely.\"\"\"\n",
        "    if b == 0:\n",
        "        raise ValueError(\"Division by zero is not allowed.\")\n",
        "    return a / b\n",
        "\n",
        "parameters = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"a\": {\"type\": \"number\", \"description\": \"Numerator\"},\n",
        "        \"b\": {\"type\": \"number\", \"description\": \"Denominator\"}\n",
        "    },\n",
        "    \"required\": [\"a\", \"b\"]\n",
        "}\n",
        "\n",
        "def error_handler(error, run_context, agent):\n",
        "    return {\"error\": str(error)}\n",
        "\n",
        "divide_tool = FunctionTool(\n",
        "    name=\"safe_divide\",\n",
        "    description=\"Safely divides two numbers.\",\n",
        "    parameters=parameters,\n",
        "    function=lambda args: safe_divide(**json.loads(args)),\n",
        "    failure_error_function=error_handler\n",
        ")\n",
        "\n",
        "# Example call\n",
        "print(divide_tool.function('{\"a\": 10, \"b\": 2}'))  # 5.0\n",
        "print(divide_tool.function('{\"a\": 10, \"b\": 0}'))  # {\"error\": \"Division by zero is not allowed.\"}\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Why Use `FunctionTool` Directly?**\n",
        "\n",
        "* **Full control** over schema, parsing, and error handling.\n",
        "* Useful if:\n",
        "\n",
        "  * You want to define tools dynamically (e.g., from config files or database).\n",
        "  * You need custom JSON parsing/validation.\n",
        "  * You want richer error messages or structured error responses.\n",
        "* But if you just want quick tools ‚Üí `@tool` decorator or `function_tool` is easier.\n",
        "\n"
      ],
      "metadata": {
        "id": "rQT0ENKbf9ZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tool-related Settings in `ModelSettings`**\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 1. **`tool_choice`**\n",
        "\n",
        "### **What It Does**\n",
        "\n",
        "Controls **whether and how the model selects tools** during a run.\n",
        "Normally, the model decides on its own when to call a tool ‚Äî `tool_choice` lets you override that.\n",
        "\n",
        "### **Options**\n",
        "\n",
        "* `\"auto\"` *(default)* ‚Üí Model chooses whether to use a tool or not.\n",
        "* `\"none\"` ‚Üí Tools are **disabled** (model can only generate text).\n",
        "* `\"required\"` ‚Üí model has no choice but to call atleast on tool.\n",
        "* Specific tool name ‚Üí Force the model to call that tool.\n",
        "\n",
        "### **Example**\n",
        "\n",
        "```python\n",
        "from openai.types.agent import ModelSettings\n",
        "\n",
        "# Let the model freely decide\n",
        "settings_auto = ModelSettings(tool_choice=\"auto\")\n",
        "\n",
        "# Disable tools\n",
        "settings_none = ModelSettings(tool_choice=\"none\")\n",
        "\n",
        "# Must call tools\n",
        "settings_none = ModelSettings(tool_choice=\"required\")\n",
        "\n",
        "# Force model to always use \"get_weather\"\n",
        "settings_forced = ModelSettings(tool_choice=\"get_weather\")\n",
        "```\n",
        "\n",
        "üëâ Use case:\n",
        "\n",
        "* `\"auto\"` ‚Üí normal chat with optional tool use.\n",
        "* `\"none\"` ‚Üí debugging, or if you want text-only answers.\n",
        "* `\"required\"` ‚Üí always want the model to call a specific tool (e.g., calculators, validators, database queries).\n",
        "* `\"get_weather\"` ‚Üí when you know the query **must** hit a tool.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 2. **`parallel_tool_calls`**\n",
        "\n",
        "### **What It Does**\n",
        "\n",
        "Controls if the model can call **multiple tools at the same time** in a single response.\n",
        "\n",
        "* `True` ‚Üí Model may issue **parallel tool calls**.\n",
        "* `False` (default) ‚Üí Only one tool call per step.\n",
        "\n",
        "### **Example Scenario**\n",
        "\n",
        "User asks:\n",
        "*\"What‚Äôs the weather in Paris and also convert 20 USD to EUR?\"*\n",
        "\n",
        "* With `parallel_tool_calls=False` ‚Üí\n",
        "\n",
        "  1. Model calls `get_weather(\"Paris\")`.\n",
        "  2. Waits, then calls `currency_converter(20, \"USD\", \"EUR\")`.\n",
        "\n",
        "* With `parallel_tool_calls=True` ‚Üí\n",
        "  Model can call **both tools in one go**, saving time.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Example Usage**\n",
        "\n",
        "```python\n",
        "settings_parallel = ModelSettings(\n",
        "    tool_choice=\"auto\",\n",
        "    parallel_tool_calls=True\n",
        ")\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "| Setting               | Purpose                           | Values                                    | Example Use                                       |\n",
        "| --------------------- | --------------------------------- | ----------------------------------------- | ------------------------------------------------- |\n",
        "| `tool_choice`         | Decide if/which tool is used      | `\"auto\"`, `\"none\"`, or specific tool name | Force `\"get_weather\"` or disable tools            |\n",
        "| `parallel_tool_calls` | Allow multiple tool calls at once | `True` / `False`                          | Run `get_weather` + `currency_converter` together |\n",
        "\n",
        "---\n",
        "\n",
        "üëâ In short:\n",
        "\n",
        "* **`tool_choice` = control tool selection** (auto, none, or fixed).\n",
        "* **`parallel_tool_calls` = allow multitasking with tools**.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "### **These two settings control how the model uses tools**"
      ],
      "metadata": {
        "id": "xJ4MfczzgSeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`tool_use_behavior`**\n",
        "It controls what the **agent does after the model calls a tool**.\n",
        "\n",
        "\n",
        "```python\n",
        "tool_use_behavior: (\n",
        "    Literal[\"run_llm_again\", \"stop_on_first_tool\"]\n",
        "    | StopAtTools\n",
        "    | ToolsToFinalOutputFunction\n",
        ") = \"run_llm_again\"\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **What It Means**\n",
        "\n",
        "This setting decides **the flow after a tool is executed**:\n",
        "\n",
        "* Should the LLM run again with tool output?\n",
        "* Or should the agent stop immediately and return?\n",
        "* Or should we apply a custom behavior?\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Options**\n",
        "\n",
        "### 1. **`\"run_llm_again\"`** (default)\n",
        "\n",
        "* After running a tool, the agent **feeds the tool‚Äôs result back into the model**.\n",
        "* Model then decides the **final answer** (or maybe call another tool).\n",
        "\n",
        "üëâ This is the **normal iterative loop**:\n",
        "`User ‚Üí Model ‚Üí Tool ‚Üí Model again ‚Üí Final Output`.\n",
        "\n",
        "‚úÖ Best for multi-step reasoning (e.g., weather lookup + currency conversion).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### 2. **`\"stop_on_first_tool\"`**\n",
        "\n",
        "* As soon as a tool is called, the agent **stops immediately**.\n",
        "* The tool‚Äôs raw output is returned as the **final answer**.\n",
        "\n",
        "üëâ Use this if:\n",
        "\n",
        "* You want tools to be **final authority**.\n",
        "* No need to let the model ‚Äúsummarize‚Äù or ‚Äúpost-process‚Äù tool results.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### 3. **`StopAtTools`**\n",
        "\n",
        "* A **custom class** that lets you define **which tools should stop the run**.\n",
        "* Example:\n",
        "\n",
        "  * If `tool=A` is called ‚Üí stop and return immediately.\n",
        "  * If `tool=B` is called ‚Üí still feed result back into the model.\n",
        "\n",
        "üëâ Gives **fine-grained control** instead of global behavior.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### 4. **`ToolsToFinalOutputFunction`**\n",
        "\n",
        "* Lets you provide a **custom function** that takes the **tool outputs** and directly produces the **final answer**, skipping another LLM run.\n",
        "* Example: aggregate multiple tool outputs into one structured response.\n",
        "\n",
        "üëâ Good for cases where:\n",
        "\n",
        "* You want to **format or merge tool results** programmatically.\n",
        "* Example: fetch weather + news ‚Üí combine into a JSON summary without LLM reprocessing.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Example Scenarios**\n",
        "\n",
        "### `\"run_llm_again\"`\n",
        "\n",
        "```python\n",
        "User: \"What‚Äôs the weather in Paris and translate it to French?\"\n",
        "‚Üí Model calls get_weather(\"Paris\")\n",
        "‚Üí Tool returns \"20¬∞C and sunny\"\n",
        "‚Üí Model runs again ‚Üí \"√Ä Paris, il fait 20¬∞C et ensoleill√©.\"\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### `\"stop_on_first_tool\"`\n",
        "\n",
        "```python\n",
        "User: \"What‚Äôs the weather in Paris?\"\n",
        "‚Üí Model calls get_weather(\"Paris\")\n",
        "‚Üí Tool returns \"20¬∞C and sunny\"\n",
        "‚Üí Agent stops ‚Üí \"20¬∞C and sunny\" (no LLM refinement)\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### `StopAtTools`\n",
        "\n",
        "```python\n",
        "stopper = StopAtTools([\"get_weather\"])\n",
        "\n",
        "settings = ModelSettings(tool_use_behavior=stopper)\n",
        "```\n",
        "\n",
        "* If model calls `get_weather` ‚Üí stop immediately.\n",
        "* If model calls `currency_converter` ‚Üí feed back into LLM.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### `ToolsToFinalOutputFunction`\n",
        "\n",
        "```python\n",
        "def custom_merger(tool_outputs):\n",
        "    return {\"summary\": \"Merged final data\", \"details\": tool_outputs}\n",
        "\n",
        "settings = ModelSettings(tool_use_behavior=custom_merger)\n",
        "```\n",
        "\n",
        "* Instead of running LLM again, agent just calls `custom_merger` on the tool outputs.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Summary**\n",
        "\n",
        "| Option                       | Behavior                                 | Best for                      |\n",
        "| ---------------------------- | ---------------------------------------- | ----------------------------- |\n",
        "| `\"run_llm_again\"`            | Default: run LLM again after tool output | Multi-step reasoning          |\n",
        "| `\"stop_on_first_tool\"`       | Stop immediately after first tool call   | Tools as final authority      |\n",
        "| `StopAtTools`                | Stop only for certain tools              | Fine-grained control          |\n",
        "| `ToolsToFinalOutputFunction` | Use custom function to finalize output   | Structured / merged responses |\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "üëâ In short:\n",
        "**`tool_use_behavior` = defines what happens after a tool is called (loop again, stop, or custom handling).**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g8Xt9XxxlGTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`reset_tool_choice`**\n",
        "\n",
        "## **What It Does**\n",
        "\n",
        "* Controls whether the **model‚Äôs tool choice is reset after every turn** in the conversation.\n",
        "* In other words: if the model used a specific tool in one step, should that choice ‚Äústick‚Äù for the next step, or should it be cleared so the model can freely decide again?\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Behavior**\n",
        "\n",
        "### If `reset_tool_choice=True` (default)\n",
        "\n",
        "* After each tool call, the tool choice is **reset back to normal** (`auto`).\n",
        "* The model gets another chance to choose freely between tools or text.\n",
        "* Prevents the model from being ‚Äúlocked‚Äù into a single tool for the whole conversation.\n",
        "\n",
        "üëâ Best for **multi-step reasoning** where different tools may be needed sequentially.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### If `reset_tool_choice=False`\n",
        "\n",
        "* The model‚Äôs tool choice is **persisted** across steps.\n",
        "* If you forced a tool (via `tool_choice`), it will keep being used automatically unless you override it.\n",
        "* More ‚Äústicky‚Äù behavior ‚Äî useful when you want the model to **stay locked** on one tool.\n",
        "\n",
        "üëâ Best for cases like:\n",
        "\n",
        "* A **calculator agent** that should *always* keep calling the calculator.\n",
        "* When you don‚Äôt want the model to ‚Äúwander‚Äù back to free text output.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## ‚úÖ Example\n",
        "\n",
        "```python\n",
        "from openai.types.agent import ModelSettings\n",
        "\n",
        "# Reset tool choice after each step (default)\n",
        "settings_reset = ModelSettings(\n",
        "    tool_choice=\"auto\",\n",
        "    reset_tool_choice=True\n",
        ")\n",
        "\n",
        "# Persist tool choice across steps\n",
        "settings_sticky = ModelSettings(\n",
        "    tool_choice=\"get_weather\",\n",
        "    reset_tool_choice=False\n",
        ")\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Example Run**\n",
        "\n",
        "### With `reset_tool_choice=True`\n",
        "\n",
        "User: *\"What‚Äôs the weather in Paris, and also convert 20 USD to EUR?\"*\n",
        "\n",
        "* Step 1: Model calls `get_weather(\"Paris\")`.\n",
        "* Step 2: Tool choice resets ‚Üí model can now call `currency_converter(20, \"USD\", \"EUR\")`.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### With `reset_tool_choice=False`\n",
        "\n",
        "User: *\"What‚Äôs the weather in Paris, and also convert 20 USD to EUR?\"*\n",
        "\n",
        "* Step 1: Model calls `get_weather(\"Paris\")`.\n",
        "* Step 2: Tool choice does **not** reset ‚Üí model will still try to call `get_weather` again, even though conversion is needed.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Summary**\n",
        "\n",
        "| Setting                   | Behavior                            | Use Case                           |\n",
        "| ------------------------- | ----------------------------------- | ---------------------------------- |\n",
        "| `reset_tool_choice=True`  | Tool choice cleared after each step | Multi-tool, multi-step reasoning   |\n",
        "| `reset_tool_choice=False` | Tool choice persists                | Force agent to stick with one tool |\n",
        "\n",
        "---\n",
        "\n",
        "üëâ In short:\n",
        "**`reset_tool_choice` controls whether the agent ‚Äúforgets‚Äù its last tool choice or keeps reusing it.**"
      ],
      "metadata": {
        "id": "7bJL7gj4qm4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Agent as a Tool**\n",
        "\n",
        "## **What It Means**\n",
        "\n",
        "* Normally, tools are **functions** you give to an agent.\n",
        "* But you can also **wrap one Agent as a Tool** ‚Äî so another Agent (or parent Agent) can call it.\n",
        "* This allows you to build **modular systems**:\n",
        "\n",
        "  * A **main agent** that delegates specialized tasks to **sub-agents**.\n",
        "  * Example:\n",
        "\n",
        "    * Main agent = orchestrator.\n",
        "    * Sub-agent 1 = math solver.\n",
        "    * Sub-agent 2 = research summarizer.\n",
        "\n",
        "üëâ Agents become **composable building blocks**.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# **`as_tool` Method**\n",
        "\n",
        "```python\n",
        "def as_tool(\n",
        "    self,\n",
        "    tool_name: str | None,\n",
        "    tool_description: str | None,\n",
        "    custom_output_extractor: Callable[[RunResult], Awaitable[str]] | None = None,\n",
        "    is_enabled: bool\n",
        "    | Callable[[RunContextWrapper[Any], AgentBase[Any]], MaybeAwaitable[bool]] = True,\n",
        ") -> Tool:\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Parameters**\n",
        "\n",
        "### 1. `tool_name: str | None`\n",
        "\n",
        "* The name under which this agent will appear as a tool.\n",
        "* If `None`, defaults to the agent‚Äôs own name.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### 2. `tool_description: str | None`\n",
        "\n",
        "* A natural-language description of what this ‚Äúagent-tool‚Äù does.\n",
        "* Helps the **calling model** know when to use it.\n",
        "* Example: `\"A specialized agent that solves math word problems step by step.\"`\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### 3. `custom_output_extractor: Callable[[RunResult], Awaitable[str]] | None`\n",
        "\n",
        "‚ö° **Most important & tricky one**.\n",
        "\n",
        "* By default, when you run an agent, you get a `RunResult` object (which contains lots of metadata: messages, tool calls, final text, etc.).\n",
        "* But when another agent calls this one as a **tool**, the LLM only expects a **string output** (like a normal tool result).\n",
        "* `custom_output_extractor` = a function that takes a `RunResult` and extracts the part you want to return as the tool‚Äôs output.\n",
        "\n",
        "üëâ Why needed?\n",
        "\n",
        "* Because `RunResult` may contain more than just plain text: logs, intermediate steps, tool outputs, reasoning traces, etc.\n",
        "* You get to decide:\n",
        "\n",
        "  * Return just `result.output_text` (final text).\n",
        "  * Return structured JSON.\n",
        "  * Return a subset of the run (e.g., only the first message).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### 4. `is_enabled: bool | Callable[...]`\n",
        "\n",
        "* Whether this agent-tool is available.\n",
        "* Can be:\n",
        "\n",
        "  * `True` ‚Üí always available.\n",
        "  * `False` ‚Üí disabled.\n",
        "  * Callable ‚Üí dynamically decide availability at runtime.\n",
        "* Same logic as for normal tools.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# **Example: Agent as Tool**\n",
        "\n",
        "```python\n",
        "from openai import Agent\n",
        "\n",
        "# Define a sub-agent\n",
        "math_agent = Agent(\n",
        "    name=\"math_solver\",\n",
        "    model=\"gpt-4.1\",\n",
        ")\n",
        "\n",
        "# Wrap math_agent as a tool\n",
        "math_tool = math_agent.as_tool(\n",
        "    tool_name=\"solve_math\",\n",
        "    tool_description=\"Solves complex math problems step by step.\",\n",
        "    custom_output_extractor=lambda run_result: run_result.output_text\n",
        ")\n",
        "\n",
        "# Main orchestrator agent\n",
        "orchestrator = Agent(\n",
        "    model=\"gpt-4.1\",\n",
        "    tools=[math_tool]\n",
        ")\n",
        "\n",
        "# Ask the orchestrator, which will delegate to math_agent\n",
        "response = orchestrator.run(\"What is (25 * 12) + 300?\")\n",
        "print(response.output_text)\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **How `custom_output_extractor` Works**\n",
        "\n",
        "Say the `math_agent` produces this `RunResult`:\n",
        "\n",
        "```python\n",
        "RunResult(\n",
        "    output_text=\"The answer is 600.\",\n",
        "    messages=[...],\n",
        "    tool_calls=[...],\n",
        "    metadata={\"steps_taken\": 3}\n",
        ")\n",
        "```\n",
        "\n",
        "* If `custom_output_extractor=lambda r: r.output_text` ‚Üí\n",
        "  Tool output = `\"The answer is 600.\"`\n",
        "\n",
        "* If `custom_output_extractor=lambda r: json.dumps(r.metadata)` ‚Üí\n",
        "  Tool output = `{\"steps_taken\": 3}`\n",
        "\n",
        "üëâ This flexibility is what makes **agent-as-tool** so powerful.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Summary**\n",
        "\n",
        "| Parameter                 | Purpose                                                                          |\n",
        "| ------------------------- | -------------------------------------------------------------------------------- |\n",
        "| `tool_name`               | Name of this agent when exposed as a tool                                        |\n",
        "| `tool_description`        | Human-readable description for the model                                         |\n",
        "| `custom_output_extractor` | Defines **what part of RunResult is returned** when another agent calls this one |\n",
        "| `is_enabled`              | Controls availability (static or dynamic)                                        |\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "üëâ In short:\n",
        "\n",
        "* `as_tool` lets you **nest agents inside each other**.\n",
        "* `custom_output_extractor` = the ‚Äúfilter‚Äù that turns a **rich RunResult** into the **simple tool output** the calling model expects.\n",
        "\n"
      ],
      "metadata": {
        "id": "ySaP0hijDkf8"
      }
    }
  ]
}