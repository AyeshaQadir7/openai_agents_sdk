{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYF2V6NJouGa"
      },
      "source": [
        "üìô [Open in Colab][1]\n",
        "\n",
        "[1]: https://colab.research.google.com/drive/1Mpwhfm6Oyn__kwzYpTNvXJ9I7eeqdDMn?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDGeygtrEIX6"
      },
      "source": [
        "# **Context** in LLMs & Agents\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **What** is Context?\n",
        "\n",
        "* **Context** = the **information the model has access to when generating a response**.\n",
        "* This includes:\n",
        "\n",
        "  1. **Conversation history** (previous messages).\n",
        "  2. **System instructions** (e.g., ‚ÄúYou are a helpful tutor‚Äù).\n",
        "  3. **User‚Äôs latest prompt/question**.\n",
        "  4. **Extra knowledge** (e.g., from RAG, memory, or tools).\n",
        "\n",
        "üëâ Think of **context** as the **‚Äúshort-term memory‚Äù** of the AI during a conversation.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 2. **How Context Works in LLMs**\n",
        "\n",
        "* LLMs don‚Äôt actually ‚Äúremember‚Äù past chats by themselves.\n",
        "* Instead, **all relevant history is re-sent to the model** with every new request.\n",
        "* The model generates answers based only on **the given context window**.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 3. **Context Window**\n",
        "\n",
        "* The **context window** = the maximum number of tokens (chunks of text) the model can consider at once.\n",
        "* Example:\n",
        "\n",
        "  * GPT-4.1 has \\~128k token context ‚Üí \\~300 pages of text.\n",
        "  * GPT-4o-mini has \\~128k tokens too.\n",
        "\n",
        "üëâ If your conversation or documents are **longer than the window**, older parts must be **dropped or summarized**.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 4. **Context in Different Settings**\n",
        "\n",
        "1. **Chatbot** ‚Üí Context = conversation history + user instructions.\n",
        "\n",
        "   * ‚ÄúHi!‚Äù ‚Üí ‚ÄúHello, how can I help you?‚Äù\n",
        "   * If context lost ‚Üí chatbot forgets who you are.\n",
        "\n",
        "2. **RAG (Retrieval-Augmented Generation)** ‚Üí Context = query + retrieved docs.\n",
        "\n",
        "   * You ask: ‚ÄúWhat‚Äôs our refund policy?‚Äù\n",
        "   * Retriever fetches doc ‚Üí doc inserted into context ‚Üí LLM answers.\n",
        "\n",
        "3. **Agents SDK** ‚Üí Context = messages + tool outputs + system instructions.\n",
        "\n",
        "   * The agent sees:\n",
        "\n",
        "     * User query\n",
        "     * Past steps\n",
        "     * Tool responses\n",
        "   * Decides next action.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 5. **Example (OpenAI Chat API Context)**\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a friendly tutor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Explain Newton's First Law.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"It says an object in motion stays in motion...\"},\n",
        "    {\"role\": \"user\", \"content\": \"Give me an example.\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "```\n",
        "\n",
        "üëâ The assistant answers correctly because the **previous conversation is in context**.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 6. **Why Context is Important**\n",
        "\n",
        "* ‚úÖ **Makes conversations coherent** (remembers flow).\n",
        "* ‚úÖ **Grounds answers** (via RAG, external knowledge).\n",
        "* ‚úÖ **Controls behavior** (system instructions set the role).\n",
        "* ‚ö†Ô∏è **Limited** by context window (old info gets cut off).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 7. **Cheat-Sheet**\n",
        "\n",
        "| Term               | Meaning                                                      |\n",
        "| ------------------ | ------------------------------------------------------------ |\n",
        "| Context            | Info the model has access to for answering.                  |\n",
        "| Context Window     | Max number of tokens model can consider.                     |\n",
        "| Sources of Context | Conversation history, instructions, documents, tool outputs. |\n",
        "| Limitation         | Old info is lost if it exceeds the window.                   |\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Summary**:\n",
        "**Context** is the **short-term memory** of LLMs ‚Äî the set of messages, instructions, and data the model sees at once. It is limited by the **context window**. Without context, the model cannot maintain coherent conversations or give grounded answers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz_GG2N8i5Yx"
      },
      "source": [
        "# **Context Types in LLMs & Agents**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Local Context**\n",
        "\n",
        "* **Definition**: The **immediate, short-term information** given to the model for a single request or turn.\n",
        "* Includes:\n",
        "\n",
        "  * User‚Äôs **current query**\n",
        "  * A few **recent conversation messages**\n",
        "  * Any **retrieved documents** (in RAG)\n",
        "  * **Tool outputs** relevant to that step\n",
        "\n",
        "üëâ Think of it as **the notes in front of you right now** ‚Äî only what you need to answer the current question.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "* User: *‚ÄúExplain Newton‚Äôs First Law.‚Äù*\n",
        "* Assistant: *‚ÄúIt says an object in motion stays in motion...‚Äù*\n",
        "* User: *‚ÄúGive me an example.‚Äù*\n",
        "\n",
        "  * **Local context** = ‚ÄúGive me an example‚Äù + the assistant‚Äôs last answer about Newton‚Äôs First Law.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 2. **LLM Context**\n",
        "\n",
        "* **Definition**: The **entire set of tokens** (messages, docs, instructions) that are fed into the **model‚Äôs context window** during inference.\n",
        "* Determined by the model‚Äôs **context window size** (e.g., GPT-4.1 ‚Üí 128k tokens).\n",
        "* It is the **raw input the model sees all at once**.\n",
        "\n",
        "üëâ Think of it as the **page(s) of text you can fit into your working memory at one time**.\n",
        "\n",
        "**Example**:\n",
        "If you paste a **50-page document** into GPT-4.1 and ask for a summary, all 50 pages are part of the **LLM context** (as long as they fit inside the token limit).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 3. **Agent Context**\n",
        "\n",
        "* **Definition**: The **broader operational context** that an **Agent** in the OpenAI Agents SDK has when reasoning and acting.\n",
        "* Includes:\n",
        "\n",
        "  * **System instructions** (e.g., ‚ÄúYou are a helpful tutor‚Äù).\n",
        "  * **Conversation history** (whatever is passed to the agent).\n",
        "  * **Local context** (latest query + retrieved/tool results).\n",
        "  * **Tool outputs** (like calculator results, API calls).\n",
        "  * **Agent state** (e.g., memory of decisions made in the workflow).\n",
        "\n",
        "üëâ Think of it as the **workspace of an agent**: It not only has the local text, but also knows about tools it can call, results it has seen, and its role.\n",
        "\n",
        "**Example** (Agent solving a math problem):\n",
        "\n",
        "* User: *‚ÄúWhat‚Äôs the square root of 256?‚Äù*\n",
        "* Agent context includes:\n",
        "\n",
        "  * System: *‚ÄúYou are a math solver agent.‚Äù*\n",
        "  * User‚Äôs latest question.\n",
        "  * Knowledge of available tool: *Calculator.*\n",
        "  * Tool call result: *Calculator ‚Üí 16.*\n",
        "* The agent uses this **context bundle** to decide its final answer.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 4. **Relationship Between Them**\n",
        "\n",
        "* **Local Context** ‚¨ÖÔ∏è a subset of the conversation/docs that are **immediately relevant**.\n",
        "* **LLM Context** ‚¨ÖÔ∏è the **full prompt input** (all tokens) that actually fit inside the model‚Äôs window.\n",
        "* **Agent Context** ‚¨ÖÔ∏è bigger picture ‚Üí includes **local context + tool state + workflow info**, so the agent can plan actions.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 5. **Analogy**\n",
        "\n",
        "* **Local Context** = Notes on the desk (short-term info you‚Äôre actively using).\n",
        "* **LLM Context** = Everything you can fit into your working memory at once.\n",
        "* **Agent Context** = Your entire office ‚Üí notes, tools, calculator, whiteboard, past results.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 6. **Comparison Table**\n",
        "\n",
        "| Type              | Definition                                      | Scope                                                                  | Example                                  |\n",
        "| ----------------- | ----------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------- |\n",
        "| **Local Context** | Immediate info for current turn                 | Short-term (latest query, recent msgs, retrieved docs, tool outputs)   | ‚ÄúGive me an example‚Äù + last answer       |\n",
        "| **LLM Context**   | The full input tokens inside the model‚Äôs window | Limited by token size (e.g., 128k)                                     | 50-page doc fed into GPT-4.1             |\n",
        "| **Agent Context** | The agent‚Äôs full operational workspace          | Includes system role, conversation, local context, tool calls, results | Tutor agent + memory + calculator output |\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Summary**:\n",
        "\n",
        "* **Local context** = immediate relevant info.\n",
        "* **LLM context** = the total tokens the model sees in its context window.\n",
        "* **Agent context** = the wider state available to an Agent (system role, local context, tools, results, workflow state).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8MgqlrDgn-5"
      },
      "source": [
        "## **Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZUSw_FMagNJl",
        "outputId": "9c078fc3-bc59-4359-8126-50dd0909c425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai-agents\n",
            "  Downloading openai_agents-0.2.10-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting griffe<2,>=1.5.6 (from openai-agents)\n",
            "  Downloading griffe-1.13.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: mcp<2,>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (1.13.1)\n",
            "Collecting openai<2,>=1.102.0 (from openai-agents)\n",
            "  Downloading openai-1.104.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2.10 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (2.11.7)\n",
            "Requirement already satisfied: requests<3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (2.32.4)\n",
            "Collecting types-requests<3,>=2.0 (from openai-agents)\n",
            "  Downloading types_requests-2.32.4.20250809-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (4.15.0)\n",
            "Collecting colorama>=0.4 (from griffe<2,>=1.5.6->openai-agents)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (4.10.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.4.1)\n",
            "Requirement already satisfied: httpx>=0.27.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.28.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (3.0.2)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.47.3)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.35.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.102.0->openai-agents) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.102.0->openai-agents) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.102.0->openai-agents) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.102.0->openai-agents) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.10->openai-agents) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.10->openai-agents) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.10->openai-agents) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.27.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.5.2->mcp<2,>=1.11.0->openai-agents) (1.1.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp<2,>=1.11.0->openai-agents) (8.2.1)\n",
            "Downloading openai_agents-0.2.10-py3-none-any.whl (178 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m178.9/178.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading griffe-1.13.0-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.104.2-py3-none-any.whl (928 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m928.2/928.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250809-py3-none-any.whl (20 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: types-requests, colorama, griffe, openai, openai-agents\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.101.0\n",
            "    Uninstalling openai-1.101.0:\n",
            "      Successfully uninstalled openai-1.101.0\n",
            "Successfully installed colorama-0.4.6 griffe-1.13.0 openai-1.104.2 openai-agents-0.2.10 types-requests-2.32.4.20250809\n"
          ]
        }
      ],
      "source": [
        "!pip install openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnKuuf74gRmi"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVAAESglgiqL"
      },
      "source": [
        "## **Config**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQo9dZXKgbiB"
      },
      "outputs": [],
      "source": [
        "from agents import AsyncOpenAI, OpenAIChatCompletionsModel\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key = GEMINI_API_KEY,\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    openai_client = external_client\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy0TyajXhgz_"
      },
      "outputs": [],
      "source": [
        "from agents import set_tracing_disabled\n",
        "set_tracing_disabled(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsm2SXm5hoVd"
      },
      "source": [
        "## **Local Context**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNqmbt6Qhv_9",
        "outputId": "83d24843-fb6b-4c87-b683-6c7a8acc9036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The user is 19 years old and is from Pakistan.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from agents import Agent, RunContextWrapper, Runner, function_tool\n",
        "\n",
        "@dataclass\n",
        "class UserInfo:\n",
        "  name: str\n",
        "  age: int\n",
        "  uid: int\n",
        "  location: str = \"Pakistan\"\n",
        "\n",
        "@function_tool\n",
        "async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str:\n",
        "  '''Returns the age of the user.'''\n",
        "  return f\"User {wrapper.context.name} is {wrapper.context.age} years old.\"\n",
        "\n",
        "@function_tool\n",
        "async def fetch_user_location(wrapper: RunContextWrapper[UserInfo]) -> str:\n",
        "  '''Returns the location of the user.'''\n",
        "  return f\"User {wrapper.context.name} is from {wrapper.context.location}\"\n",
        "\n",
        "async def main():\n",
        "    user_info = UserInfo(name=\"Ayesha\", uid=1234, age=19)\n",
        "\n",
        "    agent = Agent[UserInfo](\n",
        "        name= \"Assistant\",\n",
        "        tools=[fetch_user_age, fetch_user_location],\n",
        "        model = model,\n",
        "    )\n",
        "\n",
        "    result = await Runner.run(\n",
        "        starting_agent = agent,\n",
        "        input=\"What is the current age of the user? and current location?\",\n",
        "        context=user_info,\n",
        "    )\n",
        "\n",
        "    print(result.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OaupdE4nkrR",
        "outputId": "f8ad2242-3be5-4f28-e8b0-f12cc20a6e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello Ayesha, Hello! Welcome.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from agents import Agent, RunContextWrapper, Runner, function_tool\n",
        "\n",
        "@dataclass\n",
        "class UserInfo:\n",
        "  name: str\n",
        "  age: int\n",
        "  uid: int\n",
        "  location: str = \"Pakistan\"\n",
        "\n",
        "@function_tool\n",
        "async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str:\n",
        "  '''Returns the age of the user.'''\n",
        "  return f\"User {wrapper.context.name} is {wrapper.context.age} years old.\"\n",
        "\n",
        "@function_tool\n",
        "async def fetch_user_location(wrapper: RunContextWrapper[UserInfo]) -> str:\n",
        "  '''Returns the location of the user.'''\n",
        "  return f\"User {wrapper.context.name} is from {wrapper.context.location}\"\n",
        "\n",
        "@function_tool\n",
        "async def greet_user(context: RunContextWrapper[UserInfo], greeting:str) -> str:\n",
        "  \"\"\"Greet the user with thier name.\n",
        "  Args:\n",
        "  greeting: A specialized greeting message for user\n",
        "  \"\"\"\n",
        "\n",
        "  name = context.context.name\n",
        "  return f\"Hello {name}, {greeting}\"\n",
        "\n",
        "async def main():\n",
        "    user_info = UserInfo(name=\"Ayesha\", uid=1234, age=19)\n",
        "\n",
        "    agent = Agent[UserInfo](\n",
        "        name= \"Assistant\",\n",
        "        tools=[greet_user],\n",
        "        model = model,\n",
        "        instructions=\"Always greet the user using <function_call>greet_user</function_call> and welcome them\"\n",
        "    )\n",
        "\n",
        "    result = await Runner.run(\n",
        "        starting_agent = agent,\n",
        "        input=\"hello\",\n",
        "        context=user_info,\n",
        "    )\n",
        "\n",
        "    print(result.final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. **Static Instructions (the `instructions` field)**\n",
        "\n",
        "When you build an `Agent`, you give it a **base system prompt**:\n",
        "\n",
        "```python\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You are a helpful assistant\",\n",
        "    model=model,\n",
        "    tools=[get_weather],\n",
        ")\n",
        "```\n",
        "\n",
        "This `instructions` string is like the **default personality / role definition**. It stays fixed across all runs of this agent, unless you explicitly override it.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 2. **Dynamic Instructions**\n",
        "\n",
        "Sometimes you don‚Äôt want to hard-code everything into the `instructions`.\n",
        "Instead, you want to **inject context dynamically** at runtime, depending on the conversation, external signals, or function outputs.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* User preferences (tone, language, formality)\n",
        "* Current date/time or session metadata\n",
        "* Tool results (like weather, database values, app state)\n",
        "* Business rules or constraints\n",
        "\n",
        "The `agents` library allows you to pass **extra context / dynamic instructions** into each run so that the agent adapts without modifying its base config.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 3. **How it works in practice**\n",
        "\n",
        "You can add dynamic instructions when you call the runner:\n",
        "\n",
        "```python\n",
        "result = Runner.run_sync(\n",
        "    agent,\n",
        "    \"What should I wear today?\",\n",
        "    context={\n",
        "        \"dynamic_instructions\": \"The user is currently in Karachi, where it's 40¬∞C and very sunny.\"\n",
        "    }\n",
        ")\n",
        "```\n",
        "\n",
        "Inside the run, the model receives something like:\n",
        "\n",
        "```\n",
        "System: You are a helpful assistant.\n",
        "System (dynamic): The user is currently in Karachi, where it's 40¬∞C and very sunny.\n",
        "User: What should I wear today?\n",
        "```\n",
        "\n",
        "So now the agent answers with awareness of that injected context.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Difference between context vs. dynamic instructions**\n",
        "\n",
        "* **Context** ‚Üí Structured information passed to the agent runner (can include conversation history, tool outputs, metadata).\n",
        "* **Dynamic instructions** ‚Üí A special field inside `context` that lets you inject **extra system-level guidance** for just that run.\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "* `instructions` = permanent system role.\n",
        "* `dynamic_instructions` = per-run override/augmentation.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. **Why useful**\n",
        "\n",
        "* You can keep your base agent lightweight.\n",
        "* You can adapt behavior per-user or per-query.\n",
        "* You can insert real-time knowledge without retraining or hard-coding.\n",
        "\n",
        "---\n",
        "\n",
        "üëâ Example:\n",
        "\n",
        "```python\n",
        "user_profile = \"The user prefers answers in short bullet points.\"\n",
        "weather = get_weather(\"Karachi\")\n",
        "\n",
        "result = Runner.run_sync(\n",
        "    agent,\n",
        "    \"What is the weather?\",\n",
        "    context={\n",
        "        \"dynamic_instructions\": f\"{user_profile} Also, use this weather info: {weather}\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(result.final_output)\n",
        "```\n",
        "\n",
        "Now the agent will combine:\n",
        "\n",
        "* Base: *\"You are a helpful assistant\"*\n",
        "* Dynamic: *\"The user prefers answers in short bullet points. Also, use this weather info: ‚Ä¶\"*\n",
        "* User input: *\"What is the weather?\"*\n",
        "\n",
        "---\n",
        "\n",
        "‚ö° So in short:\n",
        "\n",
        "* **Context** = everything you can pass into the run.\n",
        "* **Dynamic instructions** = a special part of context that injects extra *system-level guidance* dynamically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is `context`?\n",
        "\n",
        "When you run an agent, you can pass a `context` dictionary.\n",
        "It‚Äôs a structured bag of information that‚Äôs carried alongside the conversation and affects how the agent thinks/responds.\n",
        "\n",
        "Think of it as **the runtime state/environment** the agent sees in addition to the user‚Äôs input.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "##  What goes inside `context`?\n",
        "\n",
        "Different things depending on your use case, but the framework usually treats these as meaningful:\n",
        "\n",
        "1. **Conversation state**\n",
        "\n",
        "   * Previous messages, memory, or dialogue history.\n",
        "\n",
        "2. **Dynamic instructions** (special key)\n",
        "\n",
        "   * Extra system-level guidance injected only for this run.\n",
        "   * Example: `\"dynamic_instructions\": \"The user prefers short bullet points.\"`\n",
        "\n",
        "3. **Session / user metadata**\n",
        "\n",
        "   * Things like user preferences, role, location, time zone, etc.\n",
        "   * Example: `{\"user\": {\"id\": \"123\", \"name\": \"Ali\", \"language\": \"ur\"}}`\n",
        "\n",
        "4. **External knowledge / environment info**\n",
        "\n",
        "   * Values fetched from APIs, sensors, or tools.\n",
        "   * Example: `{\"weather\": \"Sunny, 40¬∞C in Karachi\"}`\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "##  How is `context` used?\n",
        "\n",
        "When you call:\n",
        "\n",
        "```python\n",
        "result = Runner.run_sync(\n",
        "    agent,\n",
        "    \"What should I wear today?\",\n",
        "    context={\n",
        "        \"dynamic_instructions\": \"The user is in Karachi where it's 40¬∞C.\",\n",
        "        \"user\": {\"name\": \"Ali\", \"prefers_style\": \"casual\"}\n",
        "    }\n",
        ")\n",
        "```\n",
        "\n",
        "Internally, the agent runtime merges:\n",
        "\n",
        "* **Static instructions** (set when you defined the agent)\n",
        "* **Dynamic instructions** (from context)\n",
        "* **Other context fields** (which tools or templates may use)\n",
        "* **User input**\n",
        "\n",
        "So the model prompt effectively looks like:\n",
        "\n",
        "```\n",
        "System: You are a helpful assistant.\n",
        "System (dynamic): The user is in Karachi where it's 40¬∞C.\n",
        "User metadata: {\"name\": \"Ali\", \"prefers_style\": \"casual\"}\n",
        "User: What should I wear today?\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "##  Why `context` is important\n",
        "\n",
        "* Keeps **static role** separate from **dynamic signals**.\n",
        "* Lets you **inject real-time state** without rewriting the agent.\n",
        "* Makes agents more **modular** (tools or middleware can add to context automatically).\n",
        "* Enables things like **multi-user sessions** or **stateful conversations**.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **In short**:\n",
        "`context` is the structured runtime data you pass into an agent run.\n",
        "It may include **dynamic instructions**, **user/session info**, and **environment state**.\n",
        "The agent uses it to answer more accurately and in a way tailored to that moment.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
