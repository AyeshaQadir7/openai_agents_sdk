{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## [Open in Colab][1]\n",
        "\n",
        "[1]:https://colab.research.google.com/drive/1pjFGMsk_25DQBDQ1budvLsNQx_Dbk73B?usp=sharing"
      ],
      "metadata": {
        "id": "Du7Hi_xgYCvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What are Model Settings**\n",
        "\n",
        "* They are configuration options that control how the LLM (large language model) behaves when the Agent uses it. ([OpenAI GitHub][1])\n",
        "* Not every model supports all settings — capabilities vary. ([OpenAI GitHub][1])\n",
        "* They may affect things like randomness, response length, verbosity, use of tools, etc. ([OpenAI GitHub][1])\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## **Key Settings and What They Do**\n",
        "\n",
        "Here are the important fields in **ModelSettings** and what each one means:\n",
        "\n",
        "| Setting                                                       | Type / Possible Values                | What It Controls                                                                                                                                                         | Typical Use Cases / Effects |\n",
        "| ------------------------------------------------------------- | ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------- |\n",
        "| **temperature**                                               | float                                 | How \"random\" or \"creative\" responses are. Lower → more deterministic, higher → more variation. ([OpenAI GitHub][1])                                                      |                             |\n",
        "| **top\\_p**                                                    | float                                 | Nucleus sampling: restricts output to top-probability tokens whose cumulative probability is ≤ top\\_p. Another way to control randomness. ([OpenAI GitHub][1])           |                             |\n",
        "| **frequency\\_penalty**                                        | float                                 | Penalizes repeating the same content (words / phrases) in the output. Helps reduce repetition. ([OpenAI GitHub][1])                                                      |                             |\n",
        "| **presence\\_penalty**                                         | float                                 | Encourages introducing new topics (less about repeating topics already used). ([OpenAI GitHub][1])                                                                       |                             |\n",
        "| **tool\\_choice**                                              | enum / specialized type               | Which strategy or policy the agent uses for choosing tools. (e.g. whether to use a tool for certain tasks). ([OpenAI GitHub][1])                                         |                             |\n",
        "| **parallel\\_tool\\_calls**                                     | bool                                  | Whether the agent can invoke multiple tools in parallel during a single turn, or is restricted to one at a time. ([OpenAI GitHub][1])                                    |                             |\n",
        "| **truncation**                                                | `\"auto\"` or `\"disabled\"`              | Strategy for truncating inputs/responses if they get too long, to avoid overflow / too many tokens. ([OpenAI GitHub][1])                                                 |                             |\n",
        "| **max\\_tokens**                                               | int                                   | Maximum number of tokens (words/units) in the output. Limits how long agent responses can go. ([OpenAI GitHub][1])                                                       |                             |\n",
        "| **reasoning**                                                 | object / enum                         | Settings specific for reasoning models (for example how much reasoning effort to apply). ([OpenAI GitHub][2])                                                            |                             |\n",
        "| **verbosity**                                                 | literal `\"low\"`, `\"medium\"`, `\"high\"` | How detailed / wordy the responses should be. ([OpenAI GitHub][1])                                                                                                       |                             |\n",
        "| **metadata**                                                  | dict\\[str, str]                       | Extra info you attach to the request/response (for logging, tracking, etc.). Doesn’t change content but helpful operationally. ([OpenAI GitHub][1])                      |                             |\n",
        "| **store**                                                     | bool                                  | Whether to store the generated output for later retrieval (e.g. for analytics, retracing). ([OpenAI GitHub][1])                                                          |                             |\n",
        "| **include\\_usage**                                            | bool                                  | Whether to return usage information (like token usage) in the response. Useful for cost monitoring. ([OpenAI GitHub][1])                                                 |                             |\n",
        "| **response\\_include**                                         | list                                  | Other fields to include in the response (beyond output\\_text), such as log probabilities. ([OpenAI GitHub][1])                                                           |                             |\n",
        "| **top\\_logprobs**                                             | int                                   | Number of top probable tokens to return with their log probabilities. Good for analyzing uncertainties. ([OpenAI GitHub][1])                                             |                             |\n",
        "| **extra\\_query / extra\\_body / extra\\_headers / extra\\_args** | Various                               | These are more advanced: custom fields or headers sent to the LLM API. They allow tweaking behaviour not covered by standard fields. Use with care. ([OpenAI GitHub][1]) |                             |\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## **How They Interact / How to Pick Them**\n",
        "\n",
        "Here are some guidelines for choosing settings:\n",
        "\n",
        "* **Determinism vs Creativity**\n",
        "  If you want predictable, safe responses → low temperature, maybe lower top\\_p. If you want imaginative or varied content → higher temperature or looser constraints.\n",
        "\n",
        "* **Length Control**\n",
        "  Use `max_tokens` to limit response size. If inputs (context or prompts) are long, you may also need `truncation` to ensure things don’t break.\n",
        "\n",
        "* **Tool Use Behavior**\n",
        "  If your agent needs to call tools (e.g. search, API calls), `parallel_tool_calls` matters: enabling parallel calls can speed up, but maybe risk overlapping or confusing tool outputs. `tool_choice` affects how the agent decides *which* tool to use.\n",
        "\n",
        "* **Reasoning Models**\n",
        "  For models optimized for reasoning (e.g. in GPT-5 families), there are extra settings like `reasoning.effort` or preset verbosity defaults. These models may also have special behaviour, so defaults are tuned accordingly. ([OpenAI GitHub][2])\n",
        "\n",
        "* **Verbosity**\n",
        "  If responses are too long / too much detail, reduce verbosity. If you want thorough explanations, set verbosity higher.\n",
        "\n",
        "* **Monitoring and Debugging**\n",
        "  Use `metadata`, `include_usage`, `top_logprobs`, etc., to get more visibility into what the model is doing (cost, confidence, where it's uncertain).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Example Configurations**\n",
        "\n",
        "Here are two sample setups to illustrate:\n",
        "\n",
        "1. **Customer Support Bot (Safe & Consistent)**\n",
        "\n",
        "   * temperature = 0.2\n",
        "   * top\\_p = 0.9\n",
        "   * max\\_tokens = 200\n",
        "   * presence\\_penalty = 0.0\n",
        "   * frequency\\_penalty = 0.5\n",
        "   * verbosity = “low”\n",
        "\n",
        "   *Result*: The agent gives polite, consistent replies, avoids repeating itself, keeps answers concise.\n",
        "\n",
        "2. **Creative Storytelling Agent**\n",
        "\n",
        "   * temperature = 0.9\n",
        "   * top\\_p = 0.95\n",
        "   * presence\\_penalty = 0.2\n",
        "   * frequency\\_penalty = 0.0\n",
        "   * max\\_tokens = 800\n",
        "   * verbosity = “high”\n",
        "\n",
        "   *Result*: More imaginative output, more variety, longer responses, less concern for precision / terseness.\n",
        "\n",
        "\n",
        "[1]: https://openai.github.io/openai-agents-python/ref/model_settings/?utm_source=chatgpt.com \"Model settings - OpenAI Agents SDK\"\n",
        "[2]: https://openai.github.io/openai-agents-js/guides/models/?utm_source=chatgpt.com \"Models | OpenAI Agents SDK - GitHub Pages\"\n"
      ],
      "metadata": {
        "id": "kvj0cULTKJ9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is temperature?**\n",
        "\n",
        "* **Definition**: A setting that controls how **random** or **creative** a model’s responses are.\n",
        "* **Range**: `0.0` to `2.0` (most useful range is `0.0`–`1.5`).\n",
        "* **Default**: Usually around `1.0`.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **How it Works**\n",
        "\n",
        "* At each step, the model predicts the next token (word or piece of text).\n",
        "* The temperature adjusts how “flat” or “sharp” the probability distribution is:\n",
        "\n",
        "  * **Low temperature (close to 0)** → the model picks the *most likely* words. Very deterministic, focused.\n",
        "  * **High temperature (closer to 1–2)** → the model allows *less likely* words to be chosen. More variety, creativity, and sometimes randomness.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Behavior Examples**\n",
        "\n",
        "| Temperature | Behavior                          | Example Prompt: “Write a sentence about the moon.”     |\n",
        "| ----------- | --------------------------------- | ------------------------------------------------------ |\n",
        "| **0.0**     | Deterministic, factual, stable    | “The moon is Earth’s natural satellite.”               |\n",
        "| **0.3**     | Slightly varied but still safe    | “The moon orbits the Earth and reflects sunlight.”     |\n",
        "| **0.7**     | Creative, more diverse outputs    | “Under the moon’s silver glow, the night feels alive.” |\n",
        "| **1.2**     | Very imaginative, sometimes messy | “The moon whispers secret dreams to the ocean tides.”  |\n",
        "| **2.0**     | Chaotic, often incoherent         | “Moon sparkles jellyfish grammar running sideways.”    |\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **When to Use What**\n",
        "\n",
        "* **Low temperature (0–0.3)**\n",
        "\n",
        "  * Use for: Customer support, coding, factual Q\\&A, legal/medical contexts.\n",
        "  * Goal: Consistency, accuracy, reliability.\n",
        "\n",
        "* **Medium temperature (0.5–0.8)**\n",
        "\n",
        "  * Use for: Balanced outputs, like explanations, brainstorming, or tutoring.\n",
        "  * Goal: Some creativity but still grounded.\n",
        "\n",
        "* **High temperature (0.9–1.5+)**\n",
        "\n",
        "  * Use for: Poetry, storytelling, creative writing, marketing copy.\n",
        "  * Goal: Variety, originality, risk-taking.\n",
        "\n",
        "---\n",
        "\n",
        "**In short:**\n",
        "\n",
        "* **Low = predictable & safe**\n",
        "* **High = diverse & creative**\n"
      ],
      "metadata": {
        "id": "i_hTdqk2y_WG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is top-k?**\n",
        "\n",
        "* **Definition**: The model only considers the **top K most likely next words** (tokens), and ignores all others.\n",
        "* Then it picks among those K words, weighted by their probabilities.\n",
        "* **Example**:\n",
        "\n",
        "  * Suppose the model predicts next word probabilities:\n",
        "\n",
        "    * \"cat\" → 40%\n",
        "    * \"dog\" → 30%\n",
        "    * \"bird\" → 20%\n",
        "    * \"car\" → 5%\n",
        "    * \"house\" → 5%\n",
        "  * If **k = 2**, the model only keeps *\"cat\"* and *\"dog\"*, ignores the rest.\n",
        "\n",
        "👉 **Smaller k = safer, more repetitive**.\n",
        "👉 **Larger k = more diverse, more creative**.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "## **What is top-p (a.k.a. nucleus sampling)?**\n",
        "\n",
        "* **Definition**: Instead of a fixed number of words, the model chooses from the **smallest set of words whose total probability ≥ p**.\n",
        "* **p** is between `0.0` and `1.0`.\n",
        "* **Example**:\n",
        "\n",
        "  * Same probabilities: cat (40%), dog (30%), bird (20%), car (5%), house (5%).\n",
        "  * If **p = 0.8**, the model includes words until cumulative probability ≥ 80%.\n",
        "\n",
        "    * That gives: *cat + dog* (40% + 30% = 70%) → still less than 0.8\n",
        "    * Add *bird* (20%) → total = 90% ≥ 0.8 ✅\n",
        "    * So possible choices: *cat, dog, bird*.\n",
        "  * If **p = 0.95**, it might also include *car* (to reach 95%).\n",
        "\n",
        "👉 **Lower p (e.g. 0.5)** = only top words allowed, very focused.\n",
        "👉 **Higher p (e.g. 0.95)** = more variety.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Side-by-Side Comparison**\n",
        "\n",
        "| Aspect      | **Top-k**                         | **Top-p**                                        |\n",
        "| ----------- | --------------------------------- | ------------------------------------------------ |\n",
        "| Limit style | Fixed number of choices           | Dynamic set based on probability mass            |\n",
        "| Parameter   | k (integer, e.g. 10, 50)          | p (float 0–1, e.g. 0.9, 0.95)                    |\n",
        "| Behavior    | Always keeps *exactly k* options  | Keeps a *probability-based cutoff*               |\n",
        "| Use case    | Easy to understand, simple cutoff | Smarter, adapts to context probabilities         |\n",
        "| Example     | k=3 → always 3 words considered   | p=0.9 → may be 2 words in one case, 5 in another |\n",
        "\n",
        "---\n",
        "\n",
        "## **How They’re Used in Practice**\n",
        "\n",
        "* **Top-k** = often used in early NLP models, like GPT-2. Easy but rigid.\n",
        "* **Top-p** = more common in modern LLMs (like GPT-3/4/5). Gives more natural variation.\n",
        "* They can be **combined**:\n",
        "\n",
        "  * e.g., `top_k = 50`, `top_p = 0.9` → filter to the top 50, then further restrict to probability mass 90%.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "**Quick analogy:**\n",
        "\n",
        "* **Top-k** = “I’ll only choose from the top 5 restaurants, no matter what.”\n",
        "* **Top-p** = “I’ll choose from enough restaurants to cover 90% of the best options — sometimes that’s 2, sometimes 10.”\n"
      ],
      "metadata": {
        "id": "6xEtjjFwz4yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Frequency Penalty**\n",
        "\n",
        "* **What it does**: Penalizes words that the model has already used **a lot** in the response.\n",
        "* **Effect**: Reduces **repetition** of the same word/phrase.\n",
        "* **Range**: `-2.0` to `+2.0` (default = `0`).\n",
        "\n",
        "  * Higher values = stronger penalty against repeated words.\n",
        "\n",
        "**Example** (Prompt: *“Write a short poem about rain.”*):\n",
        "\n",
        "* **Penalty = 0.0** →\n",
        "  “The rain falls softly, rain falls slow, rain falls gently on the meadow.”\n",
        "  *(lots of repetition)*\n",
        "* **Penalty = 1.0** →\n",
        "  “Softly it drifts, a silver veil across the meadow’s quiet trail.”\n",
        "  *(less repetition, more variety)*\n",
        "\n",
        "👉 Use when you want cleaner text without the model looping or repeating phrases.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Presence Penalty**\n",
        "\n",
        "* **What it does**: Penalizes words that have already been mentioned **at least once**.\n",
        "* **Effect**: Encourages the model to **introduce new topics** instead of staying on the same subject.\n",
        "* **Range**: `-2.0` to `+2.0` (default = `0`).\n",
        "\n",
        "  * Higher values = stronger push to bring in new words.\n",
        "\n",
        "**Example** (Prompt: *“Tell me something about animals.”*):\n",
        "\n",
        "* **Penalty = 0.0** →\n",
        "  “Dogs are loyal animals. Cats are also popular animals. Many animals live in the wild.”\n",
        "  *(keeps repeating “animals”)*\n",
        "* **Penalty = 1.0** →\n",
        "  “Dogs are loyal companions. Cats are independent. Elephants roam the savannah, while dolphins play in the sea.”\n",
        "  *(introduces more variety, avoids reusing “animals”)*\n",
        "\n",
        "👉 Use when you want broader exploration or brainstorming.\n",
        "\n",
        "\n",
        "## Side-by-Side\n",
        "\n",
        "| Feature        | **Frequency Penalty**               | **Presence Penalty**                              |\n",
        "| -------------- | ----------------------------------- | ------------------------------------------------- |\n",
        "| Focus          | *How many times* a word is repeated | *Whether* a word has already appeared             |\n",
        "| Goal           | Reduce overuse of the same words    | Push model to explore new topics                  |\n",
        "| Example effect | Avoids loops like “rain rain rain”  | Avoids repeating concepts like “animals, animals” |\n",
        "| Good for       | Cleaner text, less redundancy       | Brainstorming, topic diversity                    |\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## When to Use\n",
        "\n",
        "* **Use Frequency Penalty**:\n",
        "\n",
        "  * If model keeps repeating words/phrases (e.g., in long outputs).\n",
        "* **Use Presence Penalty**:\n",
        "\n",
        "  * If model is stuck talking about the *same subject* instead of moving to new ideas.\n",
        "\n",
        "---\n",
        "\n",
        "👉 In short:\n",
        "\n",
        "* **Frequency penalty = don’t say the same word too many times.**\n",
        "* **Presence penalty = don’t keep talking about the same thing.**\n"
      ],
      "metadata": {
        "id": "G72fNu7a4d8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Max Tokens (max\\_tokens)**\n",
        "\n",
        "* **What it is**: A limit on **how many tokens** the model can generate in its response.\n",
        "* **Tokens** = chunks of text (≈ 1 token = \\~4 characters in English, \\~¾ of a word).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Why it matters**\n",
        "\n",
        "* Prevents **too long** outputs.\n",
        "* Helps **control cost** (since usage is billed per token).\n",
        "* Ensures responses **fit inside context window** (prompt + output).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Example**\n",
        "\n",
        "Prompt: *“Write a story about a dragon.”*\n",
        "\n",
        "* **max\\_tokens = 20** →\n",
        "  “Once there was a dragon who lived in a cave near the mountains.” *(short)*\n",
        "\n",
        "* **max\\_tokens = 200** →\n",
        "  Longer, detailed story with setting, characters, and events.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "👉 In short:\n",
        "**max\\_tokens = maximum length of the model’s answer.**\n",
        "\n",
        "Do you want me to also explain the **difference between `max_tokens` and context window size** (common confusion)?\n"
      ],
      "metadata": {
        "id": "mr0EWPdhPBkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reasoning in ModelSettings**\n",
        "\n",
        "The **`Reasoning`** class is a special set of options that apply **only to reasoning models** (like the `o1` family, sometimes called **o-series**).\n",
        "\n",
        "\n",
        "These models are designed to handle **multi-step reasoning**, problem-solving, and structured thought more explicitly than regular models.\n",
        "\n",
        "Here’s the breakdown of its fields:\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 1. `effort`\n",
        "\n",
        "```python\n",
        "effort: Optional[ReasoningEffort] = None\n",
        "```\n",
        "\n",
        "* **What it is**: Controls how much “reasoning effort” the model applies before giving an answer.\n",
        "* **Values**:\n",
        "\n",
        "  * `\"low\"` → shorter reasoning, faster responses, fewer tokens used in hidden reasoning steps.\n",
        "  * `\"medium\"` → balanced (default-like behavior).\n",
        "  * `\"high\"` → deeper reasoning, longer “thinking”, potentially more accurate on complex tasks, but also slower & more expensive.\n",
        "\n",
        "**Analogy**:\n",
        "\n",
        "* Low effort = giving a “quick guess”\n",
        "* Medium effort = giving a “careful explanation”\n",
        "* High effort = “writing out a detailed solution like a math teacher”\n",
        "\n",
        "**When to use**:\n",
        "\n",
        "* **low** → quick checks, simple lookups, latency-sensitive tasks.\n",
        "* **medium** → general-purpose reasoning (most tasks).\n",
        "* **high** → math, logic puzzles, code correctness, deep analysis.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 2. `generate_summary` (deprecated)\n",
        "\n",
        "```python\n",
        "generate_summary: Optional[Literal[\"auto\", \"concise\", \"detailed\"]] = None\n",
        "```\n",
        "\n",
        "* **Deprecated**: use `summary` instead.\n",
        "* This was an older way to tell the model whether to output a summary of its reasoning process.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. `summary`\n",
        "\n",
        "```python\n",
        "summary: Optional[Literal[\"auto\", \"concise\", \"detailed\"]] = None\n",
        "```\n",
        "\n",
        "* **What it is**: Lets you request a **summary of the reasoning process** that the model used internally.\n",
        "* **Values**:\n",
        "\n",
        "  * `\"auto\"` → model decides how much reasoning summary to show.\n",
        "  * `\"concise\"` → short summary of reasoning.\n",
        "  * `\"detailed\"` → longer, more step-by-step summary.\n",
        "\n",
        "**Why useful**:\n",
        "\n",
        "* Debugging: you can see *how* the model arrived at an answer.\n",
        "* Transparency: helps users trust the response.\n",
        "* Teaching: useful in tutoring apps, code explanation, or logic-heavy problems.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Side-by-Side**\n",
        "\n",
        "| Field              | Purpose                              | Values                              | When to Use                         |\n",
        "| ------------------ | ------------------------------------ | ----------------------------------- | ----------------------------------- |\n",
        "| `effort`           | Controls depth of reasoning          | `\"low\"`, `\"medium\"`, `\"high\"`       | Trade-off between speed vs accuracy |\n",
        "| `generate_summary` | (Old) request reasoning summary      | `\"auto\"`, `\"concise\"`, `\"detailed\"` | Use `summary` instead               |\n",
        "| `summary`          | New way to request reasoning summary | `\"auto\"`, `\"concise\"`, `\"detailed\"` | Debugging, teaching, transparency   |\n",
        "\n",
        "---\n",
        "\n",
        "# **Example Usage**\n",
        "\n",
        "```python\n",
        "from openai import Agent\n",
        "from openai.types.agent import ModelSettings, Reasoning\n",
        "\n",
        "settings = ModelSettings(\n",
        "    reasoning=Reasoning(\n",
        "        effort=\"high\",        # ask the model to reason more deeply\n",
        "        summary=\"concise\"     # get a short explanation of reasoning\n",
        "    )\n",
        ")\n",
        "\n",
        "agent = Agent(\n",
        "    model=\"o1-preview\",  # a reasoning-capable model\n",
        "    model_settings=settings\n",
        ")\n",
        "```\n",
        "\n",
        "**Effect**:\n",
        "\n",
        "* The agent will try to spend more effort reasoning through tasks.\n",
        "* It will also provide a short summary of *how* it reached its answer.\n",
        "\n",
        "---\n",
        "\n",
        "👉 In short:\n",
        "\n",
        "* **`effort` = how hard the model thinks**\n",
        "* **`summary` = how much of that thinking you want to see**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BA1bGRocwSiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metadata**\n",
        "\n",
        "#### **What it is**\n",
        "\n",
        "* `metadata` is a **dictionary (key–value pairs)** that you can attach to a model request.\n",
        "* It **does not affect the model’s answer** — it’s purely for your **own tracking, logging, or analytics**.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Why it’s Useful**\n",
        "\n",
        "* Tag requests with **extra info** (e.g., user IDs, experiment names, session IDs).\n",
        "* Helps with **debugging** (know which request produced which output).\n",
        "* Useful for **analytics** (e.g., cost tracking, A/B testing, auditing).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Example**\n",
        "\n",
        "```python\n",
        "from openai.types.agent import ModelSettings\n",
        "\n",
        "settings = ModelSettings(\n",
        "    metadata={\n",
        "        \"user_id\": \"12345\",\n",
        "        \"session\": \"abc-xyz\",\n",
        "        \"experiment\": \"prompt_v2\"\n",
        "    }\n",
        ")\n",
        "```\n",
        "\n",
        "* Here, every response will carry this metadata, so you know which **user/session/experiment** the response belongs to.\n",
        "* The **model ignores this info** — it’s just returned back to you.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Key Points**\n",
        "\n",
        "* Type: `dict[str, str]` (string → string).\n",
        "* Optional (you can leave it empty).\n",
        "* Best practice: keep values **short and descriptive**.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "👉 In short:\n",
        "**`metadata` = your custom labels for tracking requests.**\n",
        "It’s like attaching sticky notes to a request so you can recognize it later.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# **Store**\n",
        "\n",
        "### **What it is**\n",
        "\n",
        "* Tells the system whether the **response should be stored** in OpenAI’s systems for later retrieval (e.g., to view history, debugging, or analytics).\n",
        "\n",
        "### **Why use it**\n",
        "\n",
        "* Helps if you want a **persistent record** of responses.\n",
        "* Useful for replaying or auditing outputs later.\n",
        "\n",
        "### **Note**\n",
        "\n",
        "* If `store = False`, responses are **not stored** (only returned immediately).\n",
        "* Some orgs disable storage for privacy/security reasons.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# **include_usage**\n",
        "\n",
        "### **What it is**\n",
        "\n",
        "* If `True`, the API response will include **token usage details**.\n",
        "\n",
        "### **What it gives**\n",
        "\n",
        "Example usage info returned:\n",
        "\n",
        "```json\n",
        "\"usage\": {\n",
        "  \"prompt_tokens\": 120,\n",
        "  \"completion_tokens\": 200,\n",
        "  \"total_tokens\": 320\n",
        "}\n",
        "```\n",
        "\n",
        "### **Why use it**\n",
        "\n",
        "* Track **costs** (billing is per token).\n",
        "* Monitor **efficiency** (see if prompts are too long).\n",
        "* Debug performance issues.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "# **response_include**\n",
        "\n",
        "```python\n",
        "response_include: list[str]\n",
        "```\n",
        "\n",
        "### **What it is**\n",
        "\n",
        "* A list of **extra fields** you want included in the response.\n",
        "* Beyond the default `output_text`, you can ask for additional info.\n",
        "\n",
        "### **Common values**\n",
        "\n",
        "* `\"logprobs\"` → returns log probabilities of tokens (confidence levels).\n",
        "* `\"metadata\"` → include metadata you attached.\n",
        "* `\"reasoning_summary\"` → if available, include reasoning summary.\n",
        "\n",
        "### **Why use it**\n",
        "\n",
        "* Debugging (see why the model chose certain tokens).\n",
        "* Research (inspect confidence & uncertainty).\n",
        "* Custom analytics (track metadata or reasoning outputs).\n",
        "\n",
        "---\n",
        "\n",
        "# **Side-by-Side**\n",
        "\n",
        "| Setting            | Type       | Purpose                                    | Example Use Case                 |\n",
        "| ------------------ | ---------- | ------------------------------------------ | -------------------------------- |\n",
        "| `store`            | bool       | Save response in system                    | Keep logs of all agent replies   |\n",
        "| `include_usage`    | bool       | Add token usage info in response           | Track costs per request          |\n",
        "| `response_include` | list\\[str] | Add extra fields (like logprobs, metadata) | Debug or analyze model decisions |\n",
        "\n",
        "---\n",
        "\n",
        "# **Example**\n",
        "\n",
        "```python\n",
        "from openai.types.agent import ModelSettings\n",
        "\n",
        "settings = ModelSettings(\n",
        "    store=True,                     # Save response for history\n",
        "    include_usage=True,              # Return token usage\n",
        "    response_include=[\"logprobs\"]    # Also include token log probabilities\n",
        ")\n",
        "```\n",
        "\n",
        "**Effect**:\n",
        "\n",
        "* Response is stored in history.\n",
        "* You’ll see usage numbers (`prompt_tokens`, `completion_tokens`).\n",
        "* You’ll also get detailed `logprobs` for each token.\n",
        "\n",
        "---\n",
        "\n",
        "👉 In short:\n",
        "\n",
        "* **`store` = save or not save responses**\n",
        "* **`include_usage` = return token counts (for billing/monitoring)**\n",
        "* **`response_include` = what extra fields you want in the response**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TSPRsATDSBQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is `response_include`?**\n",
        "\n",
        "```python\n",
        "response_include: list[ResponseIncludable] | None = None\n",
        "```\n",
        "\n",
        "* It’s a **list of extra fields** you can ask the API to include in its response.\n",
        "* By default, you only get the **output text**, but with `response_include`, you can also request **additional structured data** (e.g., logprobs, image URLs, reasoning traces).\n",
        "* Each value comes from `ResponseIncludable`.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **The Possible Values of `ResponseIncludable`**\n",
        "\n",
        "Here’s what each option means:\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 1. `\"code_interpreter_call.outputs\"`\n",
        "\n",
        "* Returns the **outputs from a code interpreter tool call** (like Python sandbox).\n",
        "* Example: If the agent executed Python code, you’d see the actual outputs (numbers, printed text, plots).\n",
        "\n",
        "👉 Useful for:\n",
        "\n",
        "* Debugging code execution in tool-using agents.\n",
        "* Displaying raw results of Python code.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 2. `\"computer_call_output.output.image_url\"`\n",
        "\n",
        "* Returns **image URLs** when the model/tool generated an image (e.g., via DALL·E).\n",
        "* Lets you access the direct image link instead of just the text response.\n",
        "\n",
        "👉 Useful for:\n",
        "\n",
        "* Agents that create images.\n",
        "* Displaying generated images in apps.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 3. `\"file_search_call.results\"`\n",
        "\n",
        "* Returns the **results of a file search tool call**.\n",
        "* Example: If the agent searched through uploaded documents, you’d get the matched passages/snippets.\n",
        "\n",
        "👉 Useful for:\n",
        "\n",
        "* Retrieval-augmented generation (RAG).\n",
        "* Document QA or enterprise knowledge bots.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 4. `\"message.input_image.image_url\"`\n",
        "\n",
        "* Returns the **image URLs of any images passed into the request**.\n",
        "* Normally, the API just consumes them silently, but this includes them back in the response.\n",
        "\n",
        "👉 Useful for:\n",
        "\n",
        "* Debugging multimodal inputs.\n",
        "* Showing which input images were actually processed.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 5. `\"message.output_text.logprobs\"`\n",
        "\n",
        "* Returns the **log probabilities** of generated tokens.\n",
        "* Logprobs = model’s confidence in each word choice.\n",
        "\n",
        "👉 Useful for:\n",
        "\n",
        "* Uncertainty estimation (detecting “low confidence” outputs).\n",
        "* Research/analysis of model behavior.\n",
        "* Advanced prompt evaluation.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## 6. `\"reasoning.encrypted_content\"`\n",
        "\n",
        "* Returns an **encrypted version of the model’s internal reasoning trace**.\n",
        "* This is a safety/privacy measure: you can’t directly see the chain of thought, but the encrypted blob can be used by OpenAI for diagnostics.\n",
        "\n",
        "👉 Useful for:\n",
        "\n",
        "* Debugging reasoning models.\n",
        "* Compliance (auditability without exposing raw reasoning).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Example Usage**\n",
        "\n",
        "```python\n",
        "from openai.types.agent import ModelSettings\n",
        "\n",
        "settings = ModelSettings(\n",
        "    response_include=[\n",
        "        \"message.output_text.logprobs\",       # get token logprobs\n",
        "        \"code_interpreter_call.outputs\",      # get code execution outputs\n",
        "        \"computer_call_output.output.image_url\"  # get image URLs\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "**Effect**:\n",
        "\n",
        "* Along with the normal text output, you’ll also see:\n",
        "\n",
        "  * Token confidence scores,\n",
        "  * Results from any code execution,\n",
        "  * Links to generated images.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Summary**\n",
        "\n",
        "* **`response_include`** = “What extra stuff should I see in the response?”\n",
        "* Options (`ResponseIncludable`):\n",
        "\n",
        "  * `code_interpreter_call.outputs` → code execution results\n",
        "  * `computer_call_output.output.image_url` → image generation URLs\n",
        "  * `file_search_call.results` → file search results\n",
        "  * `message.input_image.image_url` → input image URLs\n",
        "  * `message.output_text.logprobs` → log probabilities of output tokens\n",
        "  * `reasoning.encrypted_content` → encrypted reasoning trace\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "👉 In short:\n",
        "**Default response = only text.**\n",
        "**`response_include` lets you peek under the hood (tools, images, logprobs, reasoning).**\n"
      ],
      "metadata": {
        "id": "ITN0siTuRtUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect 👌 — you pasted the **actual implementation** of `resolve` and `to_json_dict` inside `ModelSettings`. Let’s walk through it line by line in plain English.\n",
        "\n",
        "---\n",
        "\n",
        "# ⚙️ `resolve`\n",
        "\n",
        "```python\n",
        "def resolve(self, override: ModelSettings | None) -> ModelSettings:\n",
        "    \"\"\"Produce a new ModelSettings by overlaying any non-None values from the\n",
        "    override on top of this instance.\"\"\"\n",
        "```\n",
        "\n",
        "### 🎯 What it does\n",
        "\n",
        "* Creates a **new `ModelSettings` object** by combining:\n",
        "\n",
        "  1. The current settings (`self`)\n",
        "  2. Any **non-None values** from another `ModelSettings` (`override`)\n",
        "\n",
        "So basically:\n",
        "👉 **Take my current settings, replace only the fields that override has set.**\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Step by Step\n",
        "\n",
        "```python\n",
        "if override is None:\n",
        "    return self\n",
        "```\n",
        "\n",
        "* If no override is given → just return the current object.\n",
        "\n",
        "```python\n",
        "changes = {\n",
        "    field.name: getattr(override, field.name)\n",
        "    for field in fields(self)\n",
        "    if getattr(override, field.name) is not None\n",
        "}\n",
        "```\n",
        "\n",
        "* Build a dictionary of **all fields in override that are not None**.\n",
        "* Example: if override only sets `max_tokens=100`, then `changes = {\"max_tokens\": 100}`.\n",
        "\n",
        "```python\n",
        "# Handle extra_args merging specially - merge dictionaries instead of replacing\n",
        "if self.extra_args is not None or override.extra_args is not None:\n",
        "    merged_args = {}\n",
        "    if self.extra_args:\n",
        "        merged_args.update(self.extra_args)\n",
        "    if override.extra_args:\n",
        "        merged_args.update(override.extra_args)\n",
        "    changes[\"extra_args\"] = merged_args if merged_args else None\n",
        "```\n",
        "\n",
        "* Special handling for `extra_args` (since it’s a dictionary).\n",
        "* Instead of replacing it entirely, it **merges both dictionaries** (self’s + override’s).\n",
        "* If keys overlap → `override` wins.\n",
        "\n",
        "```python\n",
        "return replace(self, **changes)\n",
        "```\n",
        "\n",
        "* Uses `dataclasses.replace` to return a **new `ModelSettings` object** with the updated fields.\n",
        "* This ensures immutability (doesn’t modify the old one, creates a new one).\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Example\n",
        "\n",
        "```python\n",
        "base = ModelSettings(max_tokens=50, temperature=0.7, extra_args={\"a\": 1})\n",
        "override = ModelSettings(max_tokens=100, extra_args={\"b\": 2})\n",
        "\n",
        "resolved = base.resolve(override)\n",
        "print(resolved)\n",
        "```\n",
        "\n",
        "**Result**:\n",
        "\n",
        "* `max_tokens` becomes **100** (from override)\n",
        "* `temperature` stays **0.7** (from base, since override didn’t set it)\n",
        "* `extra_args` becomes `{\"a\": 1, \"b\": 2}` (merged dictionaries)\n",
        "\n",
        "---\n",
        "\n",
        "# ⚙️ `to_json_dict`\n",
        "\n",
        "```python\n",
        "def to_json_dict(self) -> dict[str, Any]:\n",
        "    dataclass_dict = dataclasses.asdict(self)\n",
        "\n",
        "    json_dict: dict[str, Any] = {}\n",
        "\n",
        "    for field_name, value in dataclass_dict.items():\n",
        "        if isinstance(value, BaseModel):\n",
        "            json_dict[field_name] = value.model_dump(mode=\"json\")\n",
        "        else:\n",
        "            json_dict[field_name] = value\n",
        "\n",
        "    return json_dict\n",
        "```\n",
        "\n",
        "### 🎯 What it does\n",
        "\n",
        "* Converts the `ModelSettings` dataclass into a **JSON-safe dictionary**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Step by Step\n",
        "\n",
        "1. `dataclasses.asdict(self)` → turn the dataclass into a dict.\n",
        "   Example:\n",
        "\n",
        "   ```python\n",
        "   {\"max_tokens\": 100, \"reasoning\": Reasoning(...), \"temperature\": 0.7}\n",
        "   ```\n",
        "\n",
        "2. Loop through each field:\n",
        "\n",
        "   * If the value is a **`BaseModel`** (like `Reasoning`), call `value.model_dump(mode=\"json\")` to get a JSON-compatible dict.\n",
        "   * Otherwise, just copy the value.\n",
        "\n",
        "3. Return the final dictionary, which is safe to send to the API.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Example\n",
        "\n",
        "```python\n",
        "settings = ModelSettings(\n",
        "    max_tokens=100,\n",
        "    reasoning=Reasoning(effort=\"high\", summary=\"concise\")\n",
        ")\n",
        "\n",
        "print(settings.to_json_dict())\n",
        "```\n",
        "\n",
        "**Output**:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"max_tokens\": 100,\n",
        "  \"reasoning\": {\"effort\": \"high\", \"summary\": \"concise\"},\n",
        "  \"temperature\": None,\n",
        "  ...\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 📌 Summary\n",
        "\n",
        "* **`resolve(override)`**\n",
        "  → Returns a new `ModelSettings` where values from `override` overwrite `self`, with special merging for `extra_args`.\n",
        "\n",
        "* **`to_json_dict()`**\n",
        "  → Converts the dataclass into a JSON-safe dictionary, handling nested `BaseModel` objects properly.\n",
        "\n",
        "---\n",
        "\n",
        "👉 In short:\n",
        "\n",
        "* **`resolve` = overlay + merge with defaults**\n",
        "* **`to_json_dict` = prepare for API call**\n",
        "\n",
        "---\n",
        "\n",
        "Do you want me to also show you a **before/after visual of resolve()** with a real dictionary output (so you see exactly what changes)?\n"
      ],
      "metadata": {
        "id": "DQFfow5eYxmi"
      }
    }
  ]
}